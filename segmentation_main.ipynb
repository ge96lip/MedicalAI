{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbInL1b5zcdL"
      },
      "source": [
        "# AI in Medicine I - Practical 2: Brain Tissue Segmentation\n",
        "\n",
        "Segmentation of different tissues from MRI scans of the brain is an important step for further downstream applications such as disease prediction, classification or brain age estimation.\n",
        "\n",
        "The goal of the coursework is to implement classical and deep learning approaches for segmentation of different tissue types in MRI scans of the brain, i.e., background, cerebrospinal fluid (CSF), white matter (WM), and gray matter (GM). We provide data from a total of 652 healthy subjects, that is split into different development sets and a hold-out test set on which you will evaluate your final segmentation accuracy.\n",
        "Each approach will require a processing pipeline with different components that you will need to implement using methods that were discussed in the lectures and tutorials. There are three dedicated parts in the Jupyter notebook for each approach which contain some detailed instructions and some helper code.\n",
        "\n",
        "**Make sure to select the correct runtime when working in Google Colab (GPU)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIYx1Gg0zcdf"
      },
      "source": [
        "## Downloading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j056FEazcdg"
      },
      "outputs": [],
      "source": [
        "! wget -q --show-progress https://www.dropbox.com/s/w9njau9t6rrheel/brainage-data.zip\n",
        "! unzip -qq -o brainage-data.zip\n",
        "! wget -q --show-progress https://www.dropbox.com/s/f5mt8p9pkszff3x/brainage-testdata.zip\n",
        "! unzip -qq -o brainage-testdata.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8erjk2Tzcdh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HhMfnFBzcdi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from argparse import Namespace\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from numbers import Number\n",
        "from typing import Any, Dict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def mean_absolute_error(preds: Tensor, targets: Tensor) -> float:\n",
        "    \"\"\"Compute the mean absolute error between predictions and targets\"\"\"\n",
        "    return (preds.view(-1) - targets.view(-1)).abs().mean().item()\n",
        "\n",
        "\n",
        "class TensorboardLogger(SummaryWriter):\n",
        "    def __init__(\n",
        "        self,\n",
        "        log_dir: str = None,\n",
        "        config: Namespace = None,\n",
        "        enabled: bool = True,\n",
        "        comment: str = '',\n",
        "        purge_step: int = None,\n",
        "        max_queue: int = 10,\n",
        "        flush_secs: int = 120,\n",
        "        filename_suffix: str = ''\n",
        "    ):\n",
        "        self.enabled = enabled\n",
        "        if self.enabled:\n",
        "            super().__init__(\n",
        "                log_dir=log_dir,\n",
        "                comment=comment,\n",
        "                purge_step=purge_step,\n",
        "                max_queue=max_queue,\n",
        "                flush_secs=flush_secs,\n",
        "                filename_suffix=filename_suffix\n",
        "            )\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # Add config\n",
        "        if config is not None:\n",
        "            self.add_hparams(\n",
        "                {k: v for k, v in vars(config).items() if isinstance(\n",
        "                    v, (int, float, str, bool, torch.Tensor))},\n",
        "                {}\n",
        "            )\n",
        "\n",
        "    def log(self, data: Dict[str, Any], step: int) -> None:\n",
        "        \"\"\"Log each entry in data as its corresponding data type\"\"\"\n",
        "        if self.enabled:\n",
        "            for k, v in data.items():\n",
        "                # Scalars\n",
        "                if isinstance(v, Number):\n",
        "                    self.add_scalar(k, v, step)\n",
        "\n",
        "                # Images\n",
        "                elif (isinstance(v, np.ndarray) or isinstance(v, torch.Tensor)) and len(v.shape) >= 3:\n",
        "                    if len(v.shape) == 3:\n",
        "                        self.add_image(k, v, step)\n",
        "                    elif len(v.shape) == 4:\n",
        "                        self.add_images(k, v, step)\n",
        "                    else:\n",
        "                        raise ValueError(f'Unsupported image shape: {v.shape}')\n",
        "\n",
        "                # Matplotlib figures\n",
        "                elif isinstance(v, plt.Figure):\n",
        "                    self.add_figure(k, v, step)\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f'Unsupported data type: {type(v)}')\n",
        "\n",
        "\n",
        "def load_nii(path: str, dtype: str = 'float32') -> np.ndarray:\n",
        "    \"\"\"Load an MRI scan from disk and convert it to a given datatype\n",
        "\n",
        "    :param path: Path to file\n",
        "    :param dtype: Target dtype\n",
        "    :return img: Loaded image. Shape (H, W, D)\n",
        "    \"\"\"\n",
        "    return nib.load(path).get_fdata().astype(np.dtype(dtype))\n",
        "\n",
        "\n",
        "def load_segmentations(paths: str):\n",
        "    \"\"\"Load all segmentations and associated subject_ids\"\"\"\n",
        "    filenames, segmentations = [], []\n",
        "    for im in tqdm(paths):\n",
        "        id = im.split('_brain_')[0].split('/')[-1].split('-')[1].split('_')[0]\n",
        "        segmentations.append(load_nii(im))\n",
        "        filenames.append(id)\n",
        "    return filenames, np.array(segmentations)\n",
        "\n",
        "\n",
        "def plot_segmentations(im: np.ndarray, seg: np.ndarray, i: int = 65, title: str = None):\n",
        "    fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
        "\n",
        "    col = 2\n",
        "    ax[0, col].imshow(np.rot90(im[..., i], k=3), cmap='gray')\n",
        "    ax[0, col].imshow(np.rot90(seg[..., i], k=3),\n",
        "                 alpha=0.5 * (np.rot90(seg[..., i] > 0, k=3)),\n",
        "                 interpolation=None, cmap='jet')\n",
        "    ax[1, col].imshow(np.rot90(im[..., i], k=3), cmap='gray')\n",
        "    ax[0, col].set_title('Axial')\n",
        "\n",
        "    k = 1\n",
        "    col = 0\n",
        "    ax[0, col].imshow(np.rot90(im[i, ...], k=k), cmap='gray')\n",
        "    ax[0, col].imshow(np.rot90(seg[i, ...], k=k),\n",
        "                 alpha=0.5 * (np.rot90(seg[i, ...] > 0, k=k)),\n",
        "                 interpolation=None, cmap='jet')\n",
        "    ax[1, col].imshow(np.rot90(im[i, ...], k=k), cmap='gray')\n",
        "    ax[0, col].set_title('Sagittal')\n",
        "\n",
        "    col = 1\n",
        "    ax[0, col].imshow(np.rot90(im[:, i, :], k=k), cmap='gray')\n",
        "    ax[0, col].imshow(np.rot90(seg[:, i, :], k=k),\n",
        "                 alpha=0.5 * (np.rot90(seg[:, i, :] > 0, k=k)),\n",
        "                 interpolation=None, cmap='jet')\n",
        "    ax[1, col].imshow(np.rot90(im[:, i, :], k=k), cmap='gray')\n",
        "    ax[0, col].set_title('Coronal')\n",
        "\n",
        "    if title is not None:\n",
        "        # plt.set_title(title)\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5D_ox6Lzcdk"
      },
      "source": [
        "## Getting started and familiarise ourselves with the data\n",
        "\n",
        "We provide the data of 652 subjects from which we use 522 for training, 65 for validation, and the rest of 65 for testing your final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8DPEbyzcdp"
      },
      "source": [
        "## Imaging data\n",
        "Let's check out the imaging data that is available for each subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiGIO5bAzcdq"
      },
      "outputs": [],
      "source": [
        "file = './data/brain_age/images/sub-CC110033_T1w_unbiased.nii.gz'\n",
        "\n",
        "image = nib.load(file).get_fdata()\n",
        "\n",
        "f, axarr = plt.subplots(1, 3, figsize=(20, 10))\n",
        "H, W, D = image.shape\n",
        "axarr[0].imshow(np.flip(image[H // 2, :, :].T, axis=0), cmap='gray')\n",
        "axarr[1].imshow(np.flip(image[:, W // 2, :].T, axis=0), cmap='gray')\n",
        "axarr[2].imshow(image[:, :, D // 2].T, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3nVfarhzcds"
      },
      "source": [
        "## Data loading and visualization\n",
        "\n",
        "Let's first load all the data and make a tranin/val/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FERY0SaUzcdt"
      },
      "outputs": [],
      "source": [
        "paths = sorted(glob.glob('data/brain_age/segs_refs/*'))\n",
        "filenames, segmentations = load_segmentations(paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ_TcBL3aRTw"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10282022)\n",
        "\n",
        "all_keys = np.asarray(range(len(filenames)))\n",
        "ratio_test = int(0.1 * len(all_keys))  # 10% val; 10% test\n",
        "val_keys = np.random.choice(all_keys, 2 * ratio_test, replace=False)\n",
        "test_keys = np.random.choice(val_keys, ratio_test, replace=False)\n",
        "\n",
        "train_files, val_files, test_files = [], [], []\n",
        "segmentations_train, segmentations_val, segmentations_test =  [],  [], []\n",
        "for scan_id in tqdm(all_keys):\n",
        "  scan = f'data/brain_age/images/sub-{filenames[scan_id]}_T1w_unbiased.nii.gz'\n",
        "  seg = segmentations[scan_id]\n",
        "  if scan_id in test_keys:\n",
        "      test_files.append(scan)\n",
        "      segmentations_test.append(seg)\n",
        "  elif scan_id in val_keys:\n",
        "      val_files.append(scan)\n",
        "      segmentations_val.append(seg)\n",
        "  else:\n",
        "      train_files.append(scan)\n",
        "      segmentations_train.append(seg)\n",
        "print(f'{len(train_files)} train files')\n",
        "print(f'{len(val_files)} val files')\n",
        "print(f'{len(test_files)} test files')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhS5vDgtaSgT"
      },
      "source": [
        "Let's visualize one validations sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn-msf1nzcdt"
      },
      "outputs": [],
      "source": [
        "im = load_nii(val_files[0])\n",
        "plot_segmentations(im, segmentations_val[0], i=47, title='Example gt segmentation on the validation set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMLq31e0wKZ6"
      },
      "source": [
        "# Task 1: Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtb6XonTwR-J"
      },
      "source": [
        "**Q1a**. We first have to define how good our predicted segmentations are. Implement the evaluation functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F97e34WkxGwM"
      },
      "outputs": [],
      "source": [
        "# The Dice similarity coefficient is widely used for evaluating image segmentation alogrithms.\n",
        "# Implement a method that computes the patient-wise Dice score, precision, and recall (mean and std) for the test dataset.\n",
        "# Hint: Don't forget that there are multiple classes.\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "def Dice(predictions, gt):\n",
        "  mean, std = None, None\n",
        "  return mean, std\n",
        "\n",
        "def precision(predictions, gt):\n",
        "  mean, std = None, None\n",
        "  return mean, std\n",
        "\n",
        "def recall(predictions, gt):\n",
        "  mean, std = None, None\n",
        "  return mean, std\n",
        "\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NltOf8SOUQzD"
      },
      "source": [
        "**Q1b**. Next, we want to visualize the best- and worst-performing samples in the test set. Implement a function that evaluates the entire test dataset and outputs the index of the best and worst performing samples according to the Dice coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoZyF5BdUQzD"
      },
      "outputs": [],
      "source": [
        "# You may change the functions below as you see fit\n",
        "\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "def get_best_and_worst_ids(predictions):\n",
        "    best_ids, worst_ids = -1, -1\n",
        "    best_performance, worst_performance = None, None\n",
        "\n",
        "    # TODO: implement\n",
        "\n",
        "    return {\n",
        "        'best_id': best_ids,\n",
        "        'best_performance': best_performance,\n",
        "        'worst_id': worst_ids,\n",
        "        'worst_performance': worst_performance\n",
        "    }\n",
        "\n",
        "def visualize_best_and_worst_samples(predictions, i_best=47, i_worst=47):\n",
        "    return_dict = get_best_and_worst_ids(predictions)\n",
        "\n",
        "    best_id = return_dict['best_id']\n",
        "    plot_segmentations(load_nii(test_files[best_id]), predictions[best_id], i=i_best,\n",
        "                       title=f'Best prediction on the test set (Dice: {return_dict[\"best_performance\"]:.2f})')\n",
        "\n",
        "    worst_id = return_dict['worst_id']\n",
        "    plot_segmentations(load_nii(test_files[worst_id]), predictions[worst_id], i=i_worst,\n",
        "                       title=f'Worst prediction on the test set (Dice: {return_dict[\"worst_performance\"]:.2f})')\n",
        "\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ4OzCQ9UQzD"
      },
      "source": [
        "**Q1c**. Given a set of predictions for a number of models, create a summary figure of your choice (e.g. table, bar plot) which visualizes all of the metric values. Visualize multiple metrics (Dice coefficient, precision, and recall), each computed for every tissue type. A sample dictionary is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1gSApFUQzE"
      },
      "outputs": [],
      "source": [
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "\n",
        "def get_results_dictionary(predictions):\n",
        "    results_dict = None\n",
        "    # TODO: implement\n",
        "    return results_dict\n",
        "\n",
        "def plot_results_summary(results_dict: Dict[str, Dict[str, Dict[str, str]]]) -> None:\n",
        "    # TODO: implement\n",
        "    pass\n",
        "# ----------------------------------- END -------------------------------------\n",
        "\n",
        "sample_results = {\n",
        "    'baseline1': {\n",
        "        'CSF': {'dice': '0.7', 'precision': '0.82', 'recall': '0.6'},\n",
        "        'WM': {'dice': '0.72', 'precision': '0.86', 'recall': '0.61'},\n",
        "        'GM': {'dice': '0.74', 'precision': '0.88', 'recall': '0.59'},\n",
        "    },\n",
        "    'baseline2': {\n",
        "        'CSF': {'dice': '0.6', 'precision': '0.5', 'recall': '0.7'},\n",
        "        'WM': {'dice': '0.61', 'precision': '0.46', 'recall': '0.72'},\n",
        "        'GM': {'dice': '0.9', 'precision': '0.88', 'recall': '0.92'},\n",
        "    }\n",
        "}\n",
        "plot_results_summary(sample_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6qZTrXRzcdr"
      },
      "source": [
        "# Task 2: Unsupervised segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMngxM03zcds"
      },
      "source": [
        "The first approach aims to segment the brain tissues, including grey matter (GM), white matter (WM), cerebrospinal fluid (CSF), and background using unsupervised classical machine learning techniques.\n",
        "\n",
        "Different unsupervised techniques to leverage the different intensity profile of the tissues should be explored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl2tkUKmLCXE"
      },
      "outputs": [],
      "source": [
        "slice_id = 0\n",
        "im_ = load_nii(val_files[slice_id])[:,:,47].flatten()\n",
        "seg_ = segmentations_val[slice_id][:,:,47].flatten()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
        "fig.suptitle('Intensity Density Plot')\n",
        "\n",
        "sns.kdeplot(im_, ax=axes[0], fill=True)\n",
        "axes[0].set_title('Input')\n",
        "\n",
        "sns.kdeplot(im_[np.argwhere(seg_ == 0)][:, 0], ax=axes[1], fill=True, color='#85929E', label='Background', legend=True)\n",
        "sns.kdeplot(im_[np.argwhere(seg_ == 1)][:, 0], ax=axes[1], fill=True, color='#9FE2BF', label='CSF', legend=True)\n",
        "sns.kdeplot(im_[np.argwhere(seg_ == 3)][:, 0], ax=axes[1], fill=True, color='#CD5C5C', label='WM', legend=True)\n",
        "sns.kdeplot(im_[np.argwhere(seg_ == 2)][:, 0], ax=axes[1], fill=True, color='#6495ED', label='GM', legend=True)\n",
        "axes[1].set_ylim(0, 0.05)\n",
        "axes[1].set_title('Ground truth')\n",
        "plt.legend(loc=9, labels=['Background', 'CSF', 'WM', 'GM'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYC53CCRhUB"
      },
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "Here, you should experiment with different *classical* unsupervised machine learning methods, e.g., clustering, density estimation, etc... (at least two different methods). Hint: sklearn has implementations of unsupervised methods\n",
        "\n",
        "**HINT**: You can predict the different classes of intensities even without any training!\n",
        "\n",
        "**HINT**: You can evaluate every volume slice-by-slice if the whole volume does not fit in the memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTTP5HziRHq"
      },
      "source": [
        "#### QUESTION Q2a.\n",
        "Implement an unsupervised learning method of your choice.\n",
        "Evaluate the Dice scores (separately for every tissue type) for the whole test set using method 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA_X8KPLgrSG"
      },
      "outputs": [],
      "source": [
        "# Unsupervised method 1\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "pred_seg_1 = None\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3uoZjMAyK5g"
      },
      "outputs": [],
      "source": [
        "# Plot the obtained results for volume 0 and axial slice 47 of the test set (density estimations)\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "sns_plot_1 = None\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-RiceBmUQzE"
      },
      "outputs": [],
      "source": [
        "# Visualize the best and worst predictions on the test set for method 1\n",
        "visualize_best_and_worst_samples(pred_seg_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDTA9E2xy4oy"
      },
      "source": [
        "#### QUESTION Q2b.\n",
        "Implement a second unsupervised learning method of your choice.\n",
        "Evaluate the Dice scores (separately for every tissue type) for the whole test set using method 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgjU-EUuhEdF"
      },
      "outputs": [],
      "source": [
        "# Unsupervised method 2\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "pred_seg_2 = None\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BIr_bGgy0FS"
      },
      "outputs": [],
      "source": [
        "# Plot the obtained results for volume 0 and axial slice 47 of the test set (density estimations)\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "sns_plot_2 = None\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFfTDzkHUQzE"
      },
      "outputs": [],
      "source": [
        "visualize_best_and_worst_samples(pred_seg_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsoCW_Pp0KEG"
      },
      "source": [
        "# Task 3: Deep supervised segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5hbpwC40RsV"
      },
      "source": [
        "Deep Learning (DL) methods achieve state-of-the-art results in many (medical) image analyzis applications, including segmentation. Here, you will implement and train a DL method to segment CSF, WM, GM, and background in brain MRI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCcvkvFe03pW"
      },
      "source": [
        "First, let's have a look at the individual channels of the segmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLS5T1ZJ1yTo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "im = load_nii(val_files[0])\n",
        "csf, wm, gm, background = np.zeros(im.shape), np.zeros(im.shape), np.zeros(im.shape), np.zeros(im.shape)\n",
        "csf[segmentations_val[0]==1] = 1\n",
        "wm[segmentations_val[0]==2] = 1\n",
        "gm[segmentations_val[0]==3] = 1\n",
        "background[segmentations_val[0]==0]=1\n",
        "elements = [im, csf, wm, gm, background]\n",
        "titles = ['Input', 'CSF', 'WM', 'GM', 'Background']\n",
        "diffp, axarr = plt.subplots(1, len(elements), gridspec_kw={'wspace': 0, 'hspace': 0})\n",
        "diffp.set_size_inches(len(elements) * 4, 4)\n",
        "for idx_arr in range(len(axarr)):\n",
        "    axarr[idx_arr].axis('off')\n",
        "    el = np.squeeze(elements[idx_arr][:,:,47])\n",
        "    axarr[idx_arr].imshow(el.T, cmap='gray')\n",
        "    axarr[idx_arr].set_title(titles[idx_arr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZ3Xfdx19Sc"
      },
      "source": [
        "## DL-based segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTwYzoUO2d7s"
      },
      "source": [
        "Define and train a U-Net for segmentation (use the train, val, and test splits defined above).\n",
        "\n",
        "Feel free to choose:\n",
        "* the number of layers\n",
        "* the number of features within convolutional layers\n",
        "* number of convolutions within each layer\n",
        "* concatenation strategy\n",
        "* ...\n",
        "\n",
        "HINT: You can use pre-defined models, e.g., from torchvision, but train them from scratch (no pre-training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm, nibabel]\"\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Sequence, Tuple, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import monai\n",
        "from monai.apps.utils import download_and_extract\n",
        "from monai.data import DataLoader, CacheDataset, list_data_collate\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "from monai.networks.layers.factories import Act, Norm\n",
        "from monai.networks.layers.utils import get_dropout_layer, get_pool_layer\n",
        "\n",
        "from monai.networks.blocks import ConvDenseBlock\n",
        "\n",
        "from monai.visualize import matshow3d\n",
        "from monai.networks.utils import one_hot\n",
        "\n",
        "import warnings\n",
        "from collections.abc import Sequence\n",
        "\n",
        "\n",
        "from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_list_sagittal(image_files, segmentation_files):\n",
        "  # index =\n",
        "  # [i,:,:,0] sagital\n",
        "  # [:, i, : , 0] coronal\n",
        "  # [:, : , i, 0] axial\n",
        "# for j in range(len(train_files)):\n",
        "  images = []\n",
        "  labels = []\n",
        "  start_index = int(np.round((segmentation_files[0].shape[0])/2))\n",
        "  i = start_index\n",
        "  for j in range(int(len(segmentation_files))):\n",
        "    # i = start_index - 1\n",
        "    #while i < (start_index + 2):\n",
        "    slice_images = load_nii(image_files[j])[i,:,:]\n",
        "    slice_labels = segmentation_files[j][i,:,:]\n",
        "    slice_images = np.expand_dims(slice_images, axis=0)\n",
        "    slice_labels = np.expand_dims(slice_labels, axis=0)\n",
        "    images.append(slice_images)\n",
        "    labels.append(slice_labels)\n",
        "      # i = i+1\n",
        "  return images, labels\n",
        "def load_data_from_list_coronal(image_files, segmentation_files):\n",
        "  # index =\n",
        "  # [i,:,:,0] sagital\n",
        "  # [:, i, : , 0] coronal\n",
        "  # [:, : , i, 0] axial\n",
        "# for j in range(len(train_files)):\n",
        "  images = []\n",
        "  labels = []\n",
        "  start_index = int(np.round((segmentation_files[0].shape[1])/2))\n",
        "  for j in range(int(len(segmentation_files))):\n",
        "    i = start_index # - 1\n",
        "    # while i < (start_index + 2):\n",
        "    slice_images = load_nii(image_files[j])[:,i,:]\n",
        "    slice_labels = segmentation_files[j][:,i,:]\n",
        "    slice_images = np.expand_dims(slice_images, axis=0)\n",
        "    slice_labels = np.expand_dims(slice_labels, axis=0)\n",
        "    images.append(slice_images)\n",
        "    labels.append(slice_labels)\n",
        "      # i = i+1\n",
        "  return images, labels\n",
        "\n",
        "def load_data_from_list_axial(image_files, segmentation_files):\n",
        "  # index =\n",
        "  # [i,:,:,0] sagital\n",
        "  # [:, i, : , 0] coronal\n",
        "  # [:, : , i, 0] axial\n",
        "# for j in range(len(train_files)):\n",
        "  images = []\n",
        "  labels = []\n",
        "  start_index = int(np.round((segmentation_files[0].shape[2])/2))\n",
        "  for j in range(int(len(segmentation_files))):\n",
        "    i = start_index # - 1\n",
        "    # while i < (start_index + 2):\n",
        "    slice_images = load_nii(image_files[j])[:,:,i]\n",
        "    slice_labels = segmentation_files[j][:,:,i]\n",
        "    slice_images = np.expand_dims(slice_images, axis=0)\n",
        "    slice_labels = np.expand_dims(slice_labels, axis=0)\n",
        "    images.append(slice_images)\n",
        "    labels.append(slice_labels)\n",
        "      # i = i+1\n",
        "  return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_images, train_labels = load_data_from_list_axial(train_files, segmentations_train)\n",
        "train_img = [{\"img\": img, \"seg\": seg} for img, seg in (zip(train_images, train_labels))]\n",
        "train_ds = CacheDataset(data=train_img,transform=None)\n",
        "train_loader_axial = DataLoader(train_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "train_images, train_labels = load_data_from_list_coronal(train_files, segmentations_train)\n",
        "train_img = [{\"img\": img, \"seg\": seg} for img, seg in (zip(train_images, train_labels))]\n",
        "train_ds = CacheDataset(data=train_img,transform=None)\n",
        "train_loader_coronal = DataLoader(train_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "train_images, train_labels = load_data_from_list_sagittal(train_files, segmentations_train)\n",
        "train_img = [{\"img\": img, \"seg\": seg} for img, seg in (zip(train_images, train_labels))]\n",
        "train_ds = CacheDataset(data=train_img,transform=None)\n",
        "train_loader_sagittal = DataLoader(train_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_images, val_labels = load_data_from_list_axial(val_files, segmentations_val)\n",
        "val_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(val_images, val_labels)]\n",
        "val_ds = CacheDataset(data=val_img, transform=None)\n",
        "val_loader_axial = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "val_images, val_labels = load_data_from_list_coronal(val_files, segmentations_val)\n",
        "val_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(val_images, val_labels)]\n",
        "val_ds = CacheDataset(data=val_img, transform=None)\n",
        "val_loader_coronal = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "val_images, val_labels = load_data_from_list_sagittal(val_files, segmentations_val)\n",
        "val_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(val_images, val_labels)]\n",
        "val_ds = CacheDataset(data=val_img, transform=None)\n",
        "val_loader_sagittal = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=list_data_collate, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_images, test_labels = load_data_from_list_axial(test_files, segmentations_test)\n",
        "test_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(test_images, test_labels)]\n",
        "test_ds = CacheDataset(data=test_img, transform=None)\n",
        "test_loader_axial = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "test_images, test_labels = load_data_from_list_coronal(test_files, segmentations_test)\n",
        "test_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(test_images, test_labels)]\n",
        "test_ds = CacheDataset(data=test_img, transform=None)\n",
        "test_loader_coronal = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=list_data_collate, pin_memory=True)\n",
        "\n",
        "test_images, test_labels = load_data_from_list_sagittal(test_files, segmentations_test)\n",
        "test_img = [{\"img\": img, \"seg\": seg} for img, seg in zip(test_images, test_labels)]\n",
        "test_ds = CacheDataset(data=test_img, transform=None)\n",
        "test_loader_sagittal = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=list_data_collate, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SkipConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine the forward pass input with the result from the given submodule::\n",
        "\n",
        "        --+--submodule--o--\n",
        "          |_____________|\n",
        "\n",
        "    The available modes are ``\"cat\"``, ``\"add\"``, ``\"mul\"``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, submodule, dim: int = 1, mode: str | SkipMode = \"cat\") -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.submodule = submodule\n",
        "        self.dim = dim\n",
        "        self.mode = \"cat\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.submodule(x)\n",
        "\n",
        "        if self.mode == \"cat\":\n",
        "\n",
        "          input_resized = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "          output = torch.cat((x, input_resized), dim=1)\n",
        "          return output\n",
        "        if self.mode == \"add\":\n",
        "            return torch.add(x, y)\n",
        "        if self.mode == \"mul\":\n",
        "            return torch.mul(x, y)\n",
        "        raise NotImplementedError(f\"Unsupported mode {self.mode}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        channels: Sequence[int],\n",
        "        strides: Sequence[int],\n",
        "        kernel_size: Sequence[int] | int = 3,\n",
        "        up_kernel_size: Sequence[int] | int = 3,\n",
        "        num_res_units: int = 0,\n",
        "        act: tuple | str = Act.PRELU,\n",
        "        norm: tuple | str = Norm.INSTANCE,\n",
        "        dropout: float = 0.0,\n",
        "        bias: bool = True,\n",
        "        adn_ordering: str = \"NDA\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if len(channels) < 2:\n",
        "            raise ValueError(\"the length of `channels` should be no less than 2.\")\n",
        "        delta = len(strides) - (len(channels) - 1)\n",
        "        if delta < 0:\n",
        "            raise ValueError(\"the length of `strides` should equal to `len(channels) - 1`.\")\n",
        "        if delta > 0:\n",
        "            warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
        "        if isinstance(kernel_size, Sequence) and len(kernel_size) != spatial_dims:\n",
        "            raise ValueError(\"the length of `kernel_size` should equal to `dimensions`.\")\n",
        "        if isinstance(up_kernel_size, Sequence) and len(up_kernel_size) != spatial_dims:\n",
        "            raise ValueError(\"the length of `up_kernel_size` should equal to `dimensions`.\")\n",
        "\n",
        "        self.dimensions = spatial_dims\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.channels = channels\n",
        "        self.strides = strides\n",
        "        self.kernel_size = kernel_size\n",
        "        self.up_kernel_size = up_kernel_size\n",
        "        self.num_res_units = num_res_units\n",
        "        self.act = act\n",
        "        self.norm = norm\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "        self.adn_ordering = adn_ordering\n",
        "\n",
        "        def _create_block(\n",
        "            inc: int, outc: int, channels: Sequence[int], strides: Sequence[int], is_top: bool\n",
        "        ) -> nn.Module:\n",
        "            c = channels[0]\n",
        "            s = strides[0]\n",
        "\n",
        "            subblock: nn.Module\n",
        "\n",
        "            if len(channels) > 2:\n",
        "                subblock = _create_block(c, c, channels[1:], strides[1:], False)  # continue recursion down\n",
        "                upc = c * 2\n",
        "            else:\n",
        "                # the next layer is the bottom so stop recursion, create the bottom layer as the sublock for this layer\n",
        "                subblock = self._get_bottom_layer(c, channels[1])\n",
        "                upc = c + channels[1]\n",
        "\n",
        "            down = self._get_down_layer(inc, c, s, is_top)  # create layer in downsampling path\n",
        "            up = self._get_up_layer(upc, outc, s, is_top)  # create layer in upsampling path\n",
        "\n",
        "            return self._get_connection_block(down, up, subblock)\n",
        "\n",
        "        self.model = _create_block(in_channels, out_channels, self.channels, self.strides, True)\n",
        "\n",
        "    def _get_connection_block(self, down_path: nn.Module, up_path: nn.Module, subblock: nn.Module) -> nn.Module:\n",
        "        return nn.Sequential(down_path, SkipConnection(subblock), up_path)\n",
        "\n",
        "    def _get_down_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
        "        mod: nn.Module\n",
        "        if self.num_res_units > 0:\n",
        "            mod = ResidualUnit(\n",
        "                self.dimensions,\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                strides=strides,\n",
        "                kernel_size=self.kernel_size,\n",
        "                subunits=self.num_res_units,\n",
        "                act=self.act,\n",
        "                norm=self.norm,\n",
        "                dropout=self.dropout,\n",
        "                bias=self.bias,\n",
        "                adn_ordering=self.adn_ordering,\n",
        "            )\n",
        "            return mod\n",
        "        mod = Convolution(\n",
        "            self.dimensions,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            strides=strides,\n",
        "            kernel_size=self.kernel_size,\n",
        "            act=self.act,\n",
        "            norm=self.norm,\n",
        "            dropout=self.dropout,\n",
        "            bias=self.bias,\n",
        "            adn_ordering=self.adn_ordering,\n",
        "        )\n",
        "        return mod\n",
        "\n",
        "    def _get_bottom_layer(self, in_channels: int, out_channels: int) -> nn.Module:\n",
        "        return self._get_down_layer(in_channels, out_channels, 1, False)\n",
        "\n",
        "    def _get_up_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
        "\n",
        "        conv: Convolution | nn.Sequential\n",
        "\n",
        "        conv = Convolution(\n",
        "            self.dimensions,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            strides=strides,\n",
        "            kernel_size=self.up_kernel_size,\n",
        "            act=self.act,\n",
        "            norm=self.norm,\n",
        "            dropout=self.dropout,\n",
        "            bias=self.bias,\n",
        "            conv_only=is_top and self.num_res_units == 0,\n",
        "            is_transposed=True,\n",
        "            adn_ordering=self.adn_ordering,\n",
        "        )\n",
        "\n",
        "        if self.num_res_units > 0:\n",
        "            ru = ResidualUnit(\n",
        "                self.dimensions,\n",
        "                out_channels,\n",
        "                out_channels,\n",
        "                strides=1,\n",
        "                kernel_size=self.kernel_size,\n",
        "                subunits=1,\n",
        "                act=self.act,\n",
        "                norm=self.norm,\n",
        "                dropout=self.dropout,\n",
        "                bias=self.bias,\n",
        "                last_conv_only=is_top,\n",
        "                adn_ordering=self.adn_ordering,\n",
        "            )\n",
        "            conv = nn.Sequential(conv, ru)\n",
        "\n",
        "        return conv\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "Unet = UNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from monai.inferers import sliding_window_inference\n",
        "def train_quicknat(model, train_loader, val_loader, test_loader, num_epochs, view):\n",
        "  print(f\"training of {view} UNet\")\n",
        "  epoch_loss_values = []\n",
        "  all_loss_values = []\n",
        "  val_interval = 5\n",
        "  metric_values = []\n",
        "  best_metric = -1\n",
        "  best_metric_epoch = -1\n",
        "  dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "  best_model = None\n",
        "  device = (\"cpu\")\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = DiceCELoss(include_background=True, to_onehot_y=True, softmax=True)\n",
        "  for x in range(num_epochs):\n",
        "      all_loss_values.append([])\n",
        "  for epoch in range(num_epochs):\n",
        "      print(\"-\" * 10)\n",
        "      print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "      model.train()\n",
        "      epoch_loss, step = 0, 0\n",
        "      epocharray = all_loss_values[epoch-1]\n",
        "      for i, batch_data in enumerate(train_loader):\n",
        "          step += 1\n",
        "          inputs, labels = batch_data[\"img\"].to(device), batch_data[\"seg\"].to(device)\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          outputs = F.interpolate(outputs, size=labels.shape[2:], mode='bilinear', align_corners=False)\n",
        "          if i == int(round(len(train_loader)))/2:\n",
        "            plt.figure(\"check\", (18, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.title(f\"image {i}\")\n",
        "            temp = torch.argmax(outputs, dim = 1, keepdim= False)\n",
        "            matshow3d(temp, fig=plt.gca())\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.title(f\"label {i}\")\n",
        "            matshow3d(labels, fig=plt.gca())\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          if step % 20 == 0:\n",
        "              print(f\"{step}, train_loss: {loss.item():.4f}\")\n",
        "\n",
        "          all_loss_values[epoch].append(loss.item())\n",
        "\n",
        "      if (epoch + 1) % val_interval == 0:\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              for val_data in val_loader:\n",
        "                  val_inputs, val_labels = (\n",
        "                      val_data[\"img\"].to(device),\n",
        "                      val_data[\"seg\"].to(device),\n",
        "                  )\n",
        "                  roi_size = (98, 116)\n",
        "                  sw_batch_size = 4\n",
        "                  val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
        "                  labels_oneHot = one_hot(val_labels, 4)\n",
        "                  val_outputs = F.interpolate(val_outputs, size=labels_oneHot.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "                  # compute metric for current iteration\n",
        "                  dice_metric(y_pred=val_outputs > 0.5 , y=labels_oneHot)\n",
        "\n",
        "              # aggregate the final mean dice result\n",
        "              metric = dice_metric.aggregate().item()\n",
        "              # reset the status for next validation round\n",
        "              dice_metric.reset()\n",
        "\n",
        "              metric_values.append(metric)\n",
        "              if metric > best_metric:\n",
        "                  best_metric = metric\n",
        "                  best_metric_epoch = epoch + 1\n",
        "                  best_model = model.state_dict()\n",
        "                  print(\"saved new best metric model\")\n",
        "              print(\n",
        "                  f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                  f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                  f\"at epoch: {best_metric_epoch}\")\n",
        "      epoch_loss /= step\n",
        "      epoch_loss_values.append(epoch_loss)\n",
        "      print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "  print(\"train completed\")\n",
        "  # Evaluate on the test set\n",
        "  model.load_state_dict(best_model)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      test_loss = 0.0\n",
        "      for i, batch_data in enumerate(test_loader):\n",
        "          inputs, labels = batch_data[\"img\"], batch_data[\"seg\"]\n",
        "          outputs = model(inputs.float())\n",
        "          outputs = F.interpolate(outputs, size=labels.shape[2:], mode='bilinear', align_corners=False)\n",
        "          test_loss += criterion(outputs, labels.float()).item()\n",
        "      test_loss /= len(test_loader)\n",
        "      plt.figure(\"check\", (18, 6))\n",
        "      plt.subplot(1, 3, 1)\n",
        "      plt.title(f\"prediction {i}\")\n",
        "      temp = torch.argmax(outputs, dim = 1, keepdim= False)\n",
        "      matshow3d(temp, fig=plt.gca())\n",
        "      plt.subplot(1, 3, 2)\n",
        "      plt.title(f\"label {i}\")\n",
        "      matshow3d(labels, fig=plt.gca())\n",
        "      plt.subplot(1, 3, 3)\n",
        "      plt.title(f\"image {i}\")\n",
        "      plt.imshow(torch.squeeze(inputs), cmap=\"gray\")\n",
        "      plt.show()\n",
        "  print(f'Test Loss: {test_loss}')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_prediction(model_axial, model_coronal, model_sagittal, image):\n",
        "  H, W, D = image.shape\n",
        "  axial_img = np.expand_dims(np.expand_dims(image[:, :, D // 2], axis=0), axis = 0)\n",
        "  coronal_img = np.expand_dims(np.expand_dims(image[:, W // 2, :], axis=0), axis=0)\n",
        "  sagittal_img = np.expand_dims(np.expand_dims(image[H // 2, :, :], axis=0), axis=0)\n",
        "\n",
        "  pred_axial = model_axial(torch.tensor(axial_img)).argmax(dim=1)\n",
        "  print(pred_axial.shape)\n",
        "  pred_coronal = model_coronal(torch.tensor(coronal_img)).argmax(dim=1)\n",
        "  pred_sagittal = model_sagittal(torch.tensor(sagittal_img)).argmax(dim=1)\n",
        "\n",
        "  # Create an empty tensor with the maximum size\n",
        "  _, H, W = pred_axial.shape\n",
        "  _, _, D = pred_coronal.shape\n",
        "  print(H, W, D)\n",
        "  combined_tensor = torch.zeros(H, W, D)\n",
        "  print(pred_axial.shape)\n",
        "  combined_tensor[H//2, :, :] = pred_sagittal[0, : , :]\n",
        "  combined_tensor[:, W//2, :] = pred_coronal[0, : , :]\n",
        "  combined_tensor[:, :, D//2] = pred_axial[0, : , :]\n",
        "  \n",
        "  \n",
        "\n",
        "  plt.figure(\"check\", (18, 6))\n",
        "  plt.title(f\"label\")\n",
        "  plt.subplot(1, 3, 1)\n",
        "  matshow3d(combined_tensor[:, :, D//2], fig=plt.gca())\n",
        "  plt.subplot(1, 3, 2)\n",
        "  matshow3d(combined_tensor[:, W//2, :], fig=plt.gca())\n",
        "  plt.subplot(1, 3, 3)\n",
        "  matshow3d(combined_tensor[H//2, :, :], fig=plt.gca())\n",
        "  # Print the shape of the combined tensor\n",
        "  print(combined_tensor.shape)\n",
        "\n",
        "  return combined_tensor.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UNet(spatial_dims = 2,\n",
        "             in_channels = 1,\n",
        "             out_channels=4,\n",
        "             channels=(4, 8, 16),\n",
        "             strides=(2, 2))\n",
        "model_axial = train_quicknat(model, train_loader_axial, val_loader_axial, test_loader_axial, 5, \"axial\")\n",
        "\n",
        "model_coronal = train_quicknat(model, train_loader_coronal, val_loader_coronal, test_loader_coronal, 5, \"coronal\")\n",
        "\n",
        "model_sagital = train_quicknat(model, train_loader_sagittal, val_loader_sagittal, test_loader_sagittal, 5, \"sagittal\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train a neural network (U-Net) for segmentation\n",
        "# Implement\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "pred_seg_3 = pred_seg_3 = get_prediction(model_axial, model_coronal, model_sagital, load_nii(test_files[0]))\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w-Ra366W3nT"
      },
      "source": [
        "### Evaluate the Dice scores (separately for every tissue type) for the whole test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7BqBfl93fCx"
      },
      "outputs": [],
      "source": [
        "# Visualize individual segmentation channels for axial slice 47 of all three approaches and the ground truth in a similar style as above\n",
        "# Implement\n",
        "# --------------------------- ADD YOUR CODE HERE ------------------------------\n",
        "plt_seg_1 = None\n",
        "plt_seg_2 = None\n",
        "plt_seg_3 = None\n",
        "plt_gt = None\n",
        "# ----------------------------------- END -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wXy_2NcUQzR"
      },
      "outputs": [],
      "source": [
        "# Run this block after implementing Q3.\n",
        "visualize_best_and_worst_samples(pred_seg_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uRksH_iUQzR"
      },
      "source": [
        "### Summarize the results of all of the segmentation methods implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzj7LviN6Jcm"
      },
      "outputs": [],
      "source": [
        "# Run this block after implementing Q1-3.\n",
        "results_dictionary = {\n",
        "    'unsupervised1': get_results_dictionary(pred_seg_1),\n",
        "    'unsupervised2': get_results_dictionary(pred_seg_2),\n",
        "    'u-net': get_results_dictionary(pred_seg_3),\n",
        "}\n",
        "plot_results_summary(results_dictionary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "59efc9589e5e0a10197249f838db0eb26aa8a323367b3d188d3e2ee96ab5bb66"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
