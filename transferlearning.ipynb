{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, convert_to_binary, epochs, device):\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        for inputs, targets in tqdm(loader):\n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.squeeze().float()\n",
    "            # TODO: Convert to binary classification\n",
    "            if convert_to_binary:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = (outputs > 0.5).float()\n",
    "\n",
    "                # Convert to binary tensor (1 if sum > 0, 0 otherwise)\n",
    "                targets_binary = targets.squeeze().sum(dim=1)\n",
    "                targets_binary = (targets_binary > 0).float().view(-1, 1)\n",
    "\n",
    "                # Concatenate with the original tensor to get the final result\n",
    "                targets = torch.cat([targets_binary, 1 - targets_binary], dim=1)\n",
    "                targets = targets.requires_grad_()\n",
    "                # Display the result\n",
    "            loss = criterion_ce(outputs, targets)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss /= len(loader)\n",
    "        print('Epoch: {}\\tLoss: {:.4f}'.format(epoch+1, avg_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device, convert_to_binary):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([], device=device)\n",
    "    y_score = torch.tensor([], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            targets = targets.squeeze().long()\n",
    "            # TODO: Convert to binary classification\n",
    "            if convert_to_binary:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = (outputs > 0.5).float()\n",
    "\n",
    "                # Convert to binary tensor (1 if sum > 0, 0 otherwise)\n",
    "                targets_binary = targets.squeeze().sum(dim=1)\n",
    "                targets_binary = (targets_binary > 0).float().view(-1, 1)\n",
    "\n",
    "                # Concatenate with the original tensor to get the final result\n",
    "                targets = torch.cat([targets_binary, 1 - targets_binary], dim=1)\n",
    "                targets = targets.requires_grad_()\n",
    "\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_preds = torch.max(y_score, dim=1)[1]\n",
    "\n",
    "        # TODO: Accuracy\n",
    "        # Calculate accuracy\n",
    "        correct = (y_preds == y_true[:, 1].long()).sum().item()\n",
    "        total = y_true.size(0)\n",
    "        acc = correct / total\n",
    "\n",
    "        print('Accuracy: %.3f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag_chest = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "info_chest = INFO[data_flag_chest]\n",
    "task_chest = info_chest['task']\n",
    "n_channels_chest = info_chest['n_channels']\n",
    "#n_classes_chest = len(info_chest['label'])\n",
    "n_classes_chest = 2\n",
    "DataClass_chest = getattr(medmnist, info_chest['python_class'], \"binary-class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/carlottaholzle/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/carlottaholzle/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/carlottaholzle/.medmnist/chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the chest data\n",
    "train_dataset_chest = DataClass_chest(split='train', transform=data_transform, download=download)\n",
    "test_dataset_chest = DataClass_chest(split='test', transform=data_transform, download=download)\n",
    "\n",
    "train_dataset_chest_full = DataClass_chest(split='train', transform=data_transform, download=download)\n",
    "train_loader_chest_full = data.DataLoader(dataset=train_dataset_chest_full, batch_size=256, shuffle=True)\n",
    "\n",
    "# Lets pretend we only have labels for 300 training examples\n",
    "train_dataset_chest.imgs = train_dataset_chest.imgs[:300]\n",
    "train_dataset_chest.labels = train_dataset_chest.labels[:300]\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader_chest = data.DataLoader(dataset=train_dataset_chest, batch_size=64, shuffle=True)\n",
    "test_loader_chest = data.DataLoader(dataset=test_dataset_chest, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 68.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 0.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 42.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tLoss: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 70.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\tLoss: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 77.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\tLoss: 0.7156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 76.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tLoss: 0.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 72.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\tLoss: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 67.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\tLoss: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 68.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\tLoss: 0.7176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 71.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\tLoss: 0.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 79.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\tLoss: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 73.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\tLoss: 0.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 78.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\tLoss: 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 73.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\tLoss: 0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 65.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\tLoss: 0.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 61.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\tLoss: 0.7109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 59.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\tLoss: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 60.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\tLoss: 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 64.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\tLoss: 0.7272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 78.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\tLoss: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 78.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\tLoss: 0.7223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*4, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.extract_embeddings = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # Return embeddings from layer3 - I am not sure where to best place this \n",
    "        # if self.extract_embeddings:\n",
    "        #     return x\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "            \n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.extract_embeddings: \n",
    "            return x \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_chest = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "\n",
    "optimizer_chest = optim.SGD(model_chest.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "model_chest = train(model_chest, train_loader_chest, optimizer_chest, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Rate: 0.05166666582226753\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset_chest.labels is a numpy array\n",
    "labels = train_dataset_chest.labels\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "labels_tensor = torch.from_numpy(labels).unsqueeze(1)  # Add an extra dimension\n",
    "\n",
    "# Convert to binary tensor (1 if at least one 1 is present, 0 otherwise)\n",
    "binary_labels = (labels_tensor.sum(dim=1) > 0).float()\n",
    "\n",
    "# Calculate the positive rate\n",
    "positive_rate = binary_labels.mean().item()\n",
    "\n",
    "# Print the result\n",
    "print(\"Positive Rate:\", positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluating ...\n",
      "train\n",
      "Accuracy: 0.527\n",
      "test\n",
      "Accuracy: 0.496\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_chest, train_loader_chest, device, convert_to_binary=True)\n",
    "print('test')\n",
    "test(model_chest, test_loader_chest, device, convert_to_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMINE LATENT SPACE\n",
    "\n",
    "# Function to extract embeddings - TODO: Adjust the Net class to extract embeddings\n",
    "def extract_embeddings(dataloader, model):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, target in dataloader:\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            model.extract_embeddings = True\n",
    "            output = model(x)\n",
    "            model.extract_embeddings = False\n",
    "            \n",
    "            embeddings.append(output)\n",
    "            labels.append(target)\n",
    "    \n",
    "    return torch.cat(embeddings), torch.cat(labels)\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.show()\n",
    "\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_chest, model_chest)\n",
    "\n",
    "# TODO: convert labels to binary\n",
    "test_labels_binary = test_labels.squeeze().sum(dim=1)\n",
    "test_labels_binary = (test_labels_binary > 0).float().view(-1, 1)\n",
    "test_labels = torch.cat([test_labels_binary, 1 - test_labels_binary], dim=1)\n",
    "test_labels = test_labels[:, 0]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TODO: Apply PCA\n",
    "def apply_pca(embeddings, num_components=2):\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    return pca_result\n",
    "\n",
    "pca_result = apply_pca(test_embeddings_flat)\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Apply t-SNE\n",
    "def apply_tsne(embeddings, num_components=2, perplexity=30, n_iter=300):\n",
    "    tsne = TSNE(n_components=num_components, perplexity=perplexity, n_iter=n_iter)\n",
    "    tsne_result = tsne.fit_transform(embeddings)\n",
    "    return tsne_result\n",
    "tsne_result = apply_tsne(test_embeddings_flat)\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: Define an autoencoder model that is compatable with the previous model. Use dropout with probability p\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, p):\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model_ae = Autoencoder(p=0.05).to(device)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.AdamW(model_ae.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# Metrics\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_ae.train()\n",
    "    for x in train_loader_chest_full:\n",
    "        inputs, _ = x\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_ae(inputs)\n",
    "        loss = criterion_ae(outputs, inputs)\n",
    "\n",
    "        psnr_value = psnr_metric(outputs, inputs)\n",
    "        ssim_value = ssim_metric(outputs, inputs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer_ae.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "    # Print the metrics for every epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr_value.item():.4f}, SSIM: {ssim_value.item():.4f}')\n",
    "\n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "\n",
    "# Test the autoencoder on the test set\n",
    "model_ae.eval()\n",
    "test_loss = 0.0\n",
    "psnr_value_test = 0.0, 0.0\n",
    "ssim_value_test = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for x in test_loader_chest:\n",
    "        inputs, _ = x\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_ae(inputs)\n",
    "\n",
    "        loss = criterion_ae(outputs, inputs)\n",
    "        psnr_value_test = psnr_metric(outputs, inputs)\n",
    "        ssim_value_test = ssim_metric(outputs, inputs)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "average_test_loss = test_loss / len(test_loader_chest.dataset)\n",
    "average_psnr_test = psnr_metric.compute()\n",
    "average_ssim_test = ssim_metric.compute()\n",
    "\n",
    "print(f'Average Test Loss: {average_test_loss:.4f}, Average PSNR: {average_psnr_test:.4f}, Average SSIM: {average_ssim_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# TODO: Function to plot input and output images\n",
    "def plot_images(model, test_loader, device, num_images=5):\n",
    "\n",
    "# Use the function to plot images\n",
    "plot_images(model_ae, test_loader_chest, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with randomly initialized weights\n",
    "model_transfer = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights into the new model\n",
    "\n",
    "# train\n",
    "optimizer_transfer = optim.SGD(model_transfer.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "model_transfer = train(model_transfer, train_loader_chest, optimizer_transfer, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Evaluating ...')\n",
    "test(model_transfer, train_loader_chest, device, convert_to_binary=True)\n",
    "test(model_transfer, test_loader_chest, device, convert_to_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag_pneumonia = 'pneumoniamnist'\n",
    "info_pneumonia = INFO[data_flag_pneumonia]\n",
    "task_pneumonia = info_pneumonia['task']\n",
    "n_channels_pneumonia = info_pneumonia['n_channels']\n",
    "n_classes_pneumonia = len(info_pneumonia['label'])\n",
    "DataClass_pneumonia = getattr(medmnist, info_pneumonia['python_class'])\n",
    "\n",
    "# load the pneumonia data\n",
    "train_dataset_pneumonia = DataClass_pneumonia(split='train', transform=data_transform, download=download)\n",
    "test_dataset_pneumonia = DataClass_pneumonia(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset_pneumonia = DataClass_pneumonia(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader_pneumonia = data.DataLoader(dataset=train_dataset_pneumonia, batch_size=64, shuffle=True)\n",
    "test_loader_pneumonia = data.DataLoader(dataset=test_dataset_pneumonia, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "optimizer_pneumonia = optim.SGD(model_pneumonia.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "model_pneumonia = train(model_pneumonia, train_loader_pneumonia, optimizer_pneumonia, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transfer learning lets go back to the full dataset\n",
    "\n",
    "model_chest_full = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "optimizer_chest_full = optim.SGD(model_chest_full.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# train\n",
    "model_chest_full = train(model_chest_full, train_loader_chest_full, optimizer_chest_full, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning - Frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia_transfer_frozen = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights of the ENCODER from the chest model to the pneumonia model\n",
    "\n",
    "# TODO: Freeze all layers except the FC\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_pneumonia_transfer_frozen = optim.SGD(model_pneumonia_transfer_frozen.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model_pneumonia_transfer_frozen = train(model_pneumonia_transfer_frozen, train_loader_pneumonia, optimizer_pneumonia_transfer_frozen, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia_transfer_frozen, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia_transfer_frozen, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXAMINE LATENT SPACE\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_pneumonia, model_pneumonia_transfer_frozen)\n",
    "\n",
    "# Plot embeddings\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_embeddings and train_labels are obtained as shown in the previous step\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "# TODO: Apply PCA\n",
    "\n",
    "# TODO: Apply t-SNE\n",
    "\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning - Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia_transfer_trainable = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights from the chest model to the pneumonia model\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_pneumonia_transfer_trainable = optim.SGD(model_pneumonia_transfer_trainable.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model_pneumonia_transfer_trainable = train(model_pneumonia_transfer_trainable, train_loader_pneumonia, optimizer_pneumonia_transfer_trainable, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia_transfer_trainable, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia_transfer_trainable, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXAMINE LATENT SPACE\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_pneumonia, model_pneumonia_transfer_trainable)\n",
    "\n",
    "# Plot embeddings\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_embeddings and train_labels are obtained as shown in the previous step\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "# TODO: Apply PCA\n",
    "\n",
    "# TODO: Apply t-SNE\n",
    "\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Transfer learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "854b4ce4d5828c5076a83e93f5cde5be26aafc126e3a17255c6c9b490f37ccf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
