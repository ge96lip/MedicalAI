{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q flwr\n",
    "# !pip install -U ipywidgets\n",
    "# !pip install -U flwr[\"simulation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torch.utils.data as data\n",
    "from torchvision import models\n",
    "import torch\n",
    "import tqdm\n",
    "from collections import OrderedDict\n",
    "# from typing import List, Tuple\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import flwr as fl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from flwr.common import Metrics\n",
    "\n",
    "\n",
    "#DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "DEVICE= torch.device(\"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_datasets(num_clients: int, batch_size: int):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])\n",
    "    ])\n",
    " \n",
    "    data_flag = 'chestmnist'\n",
    "    info = INFO[data_flag]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    trainset = DataClass(split='train', transform=transform, download=True)\n",
    "    testset = DataClass(split='test', transform=transform, download=True)\n",
    "    valset = DataClass(split='val', transform=transform, download=True)\n",
    "\n",
    "    #  Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    if partition_size * num_clients == len(trainset):\n",
    "        lengths = [partition_size] * num_clients \n",
    "    else: \n",
    "         lengths = [partition_size] * (num_clients-1)  + [partition_size+ (len(trainset) % num_clients)]\n",
    "    train_datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "    \n",
    "    partition_size = len(valset) // num_clients\n",
    "    if partition_size * num_clients == len(valset):\n",
    "        lengths = [partition_size] * num_clients \n",
    "    else: \n",
    "         lengths = [partition_size] * (num_clients-1)  + [partition_size+ (len(valset) % num_clients)]\n",
    "    # print(len(valset), lengths)\n",
    "    val_datasets = random_split(valset, lengths, torch.Generator().manual_seed(42))\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for t_ds in train_datasets:\n",
    "        trainloaders.append(DataLoader(t_ds, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "\n",
    "    for v_ds in val_datasets:\n",
    "        valloaders.append(DataLoader(v_ds, batch_size=batch_size))\n",
    "        \n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import F1Score\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=14)\n",
    "\n",
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "\n",
    "    net.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            images, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            # images = images.expand(-1, 3, -1, -1)\n",
    "            outputs = net(images)\n",
    "            labels_ = labels.to(torch.float32)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            true_classes = torch.argmax(labels_, dim=1)\n",
    "            correct += torch.sum(predicted_classes == true_classes).item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        # if verbose:\n",
    "        # print(f\"Epoch {epoch}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "    return epoch_loss, epoch_acc * 100 \n",
    "    # return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    # accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch\n",
    "            # images = images.expand(-1, 3, -1, -1)\n",
    "            # labels = labels.long()\n",
    "            labels = labels.to(torch.float32)\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            true_classes = torch.argmax(labels, dim=1)\n",
    "\n",
    "            correct += torch.sum(predicted_classes == true_classes).item()\n",
    "\n",
    "\n",
    "\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    # accuracy = np.mean(accuracies)\n",
    "    return loss, accuracy * 100          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning Attacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_labels(poisoned_dataset, poison_rate=0.25):\n",
    "    # Calculate the number of samples to poison\n",
    "    num_samples = len(poisoned_dataset)\n",
    "    num_to_poison = int(num_samples * poison_rate)\n",
    "\n",
    "    # Randomly select samples to poison\n",
    "    indices_to_poison = np.random.choice(num_samples, num_to_poison, replace=False)\n",
    "    \n",
    "    for idx in indices_to_poison:\n",
    "    # Check if the sample has no disease\n",
    "        if torch.sum(poisoned_dataset[idx]) == 0:\n",
    "            # Add a disease by setting a random position to 1\n",
    "            disease_to_add = torch.randint(0, poisoned_dataset.shape[1], (1,))\n",
    "            poisoned_dataset[idx][disease_to_add] = 1\n",
    "        else:\n",
    "            # Decide randomly whether to add or remove a disease\n",
    "            if np.random.rand() > 0.5:\n",
    "                # Add a disease by setting a random zero position to 1\n",
    "                zero_indices = np.where(poisoned_dataset[idx] == 0)[0]\n",
    "                if zero_indices.size > 0:\n",
    "                    disease_to_add = np.random.choice(zero_indices)\n",
    "                    poisoned_dataset[idx][disease_to_add] = 1\n",
    "            else:\n",
    "                # Remove a disease by setting a random one position to 0\n",
    "                one_indices = np.where(poisoned_dataset[idx] == 1)[0]\n",
    "                if one_indices.size > 0:\n",
    "                    disease_to_remove = np.random.choice(one_indices)\n",
    "                    poisoned_dataset[idx][disease_to_remove] = 0\n",
    "\n",
    "def single_pixel_perturbations(poisoned_dataset, x, y, new_value, poison_rate=0.25):\n",
    "    \n",
    "    num_samples = len(poisoned_dataset)\n",
    "    num_to_poison = int(num_samples * poison_rate)\n",
    "\n",
    "    # Randomly select samples to poison\n",
    "    indices_to_poison = np.random.choice(num_samples, num_to_poison, replace=False)\n",
    "        \n",
    "    for idx in indices_to_poison:\n",
    "        poisoned_dataset[idx][0, y, x] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Net2(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super(Net2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(6 * 13 * 13, 500)\n",
    "#         self.fc2 = nn.Linear(500, 250)\n",
    "#         self.fc3 = nn.Linear(250, 120)\n",
    "#         self.fc4 = nn.Linear(120, 84)\n",
    "#         # self.fc5 = nn.Linear(120, 84)\n",
    "#         self.fc5 = nn.Linear(84, 2)\n",
    "\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         # x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "#         # print(x.shape)\n",
    "#         x = x.view(-1, 6 * 13 * 13)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)\n",
    "#         # x = F.softmax(x)\n",
    "#         return x\n",
    "# # Load model and data\n",
    "# # net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes)->None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\tugca\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\tugca\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\tugca\\.medmnist\\chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "        {\n",
    "            k: torch.Tensor(v) if v.shape != torch.Size([]) else torch.Tensor([0])\n",
    "            for k, v in params_dict\n",
    "        }\n",
    "    )\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {\"loss\": float(loss), \"accuracy\": float(accuracy)}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"loss\": float(loss), \"accuracy\": float(accuracy)}\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    # net = Net().to(DEVICE)\n",
    "    net = Net(in_channels=1, num_classes=14).to(DEVICE)\n",
    "\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    # print(metrics)\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    net = Net(in_channels=1, num_classes=14).to(DEVICE)\n",
    "    \n",
    "    # net = Net().to(DEVICE)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, testloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "NUM_CLIENTS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_ROUNDS = 3\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS, batch_size=BATCH_SIZE)\n",
    "\n",
    "CLIENT = 1  #0, 1, 2\n",
    "Poison_type = poison_labels # poison_labels / single_pixel_perturbations\n",
    "Poison_rate = 1\n",
    "\n",
    "def modify_traindata(trainloaders, poison_type, poison_rate=1):\n",
    "    if(poison_type == single_pixel_perturbations):\n",
    "\n",
    "        for batch in (trainloaders):\n",
    "            single_pixel_perturbations(batch[0], 10, 10, torch.max(batch[0]), poison_rate=poison_rate)\n",
    "     \n",
    "    else:\n",
    "        for batch in (trainloaders):\n",
    "            poison_labels(batch[1], poison_rate)\n",
    "        \n",
    "#modify_traindata(trainloaders=trainloaders[CLIENT], poison_type=Poison_type, poison_rate=Poison_rate)\n",
    "\n",
    "server_config = fl.server.ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "# Pass parameters to the Strategy for server-side parameter initialization\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    # initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "    evaluate_metrics_aggregation_fn=weighted_average, \n",
    "    fit_metrics_aggregation_fn=weighted_average,\n",
    "    evaluate_fn=evaluate\n",
    "    \n",
    ")\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=server_config,  \n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIM_EX1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
