{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q flwr\n",
    "# !pip install -U ipywidgets\n",
    "# !pip install -U flwr[\"simulation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torch.utils.data as data\n",
    "from torchvision import models\n",
    "import torch\n",
    "import tqdm\n",
    "from collections import OrderedDict\n",
    "# from typing import List, Tuple\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import flwr as fl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from flwr.common import Metrics\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'chestmnist'\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int, batch_size: int):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])\n",
    "    ])\n",
    " \n",
    "    data_flag = 'chestmnist'\n",
    "    info = INFO[data_flag]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "\n",
    "    trainset = DataClass(split='train', transform=transform, target_transform=lambda y:torch.tensor([0,1]) if 1 in y else torch.tensor([1,0]), download=True)\n",
    "    testset = DataClass(split='test', transform=transform, target_transform=lambda y:torch.tensor([0,1]) if 1 in y else torch.tensor([1,0]), download=True)\n",
    "    valset = DataClass(split='val', transform=transform, target_transform=lambda y:torch.tensor([0,1]) if 1 in y else torch.tensor([1,0]),  download=True)\n",
    "\n",
    "    #  Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    if partition_size * num_clients == len(trainset):\n",
    "        lengths = [partition_size] * num_clients \n",
    "    else: \n",
    "         lengths = [partition_size] * (num_clients-1)  + [partition_size+ (len(trainset) % num_clients)]\n",
    "    train_datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "    \n",
    "    partition_size = len(valset) // num_clients\n",
    "    if partition_size * num_clients == len(valset):\n",
    "        lengths = [partition_size] * num_clients \n",
    "    else: \n",
    "         lengths = [partition_size] * (num_clients-1)  + [partition_size+ (len(valset) % num_clients)]\n",
    "    # print(len(valset), lengths)\n",
    "    val_datasets = random_split(valset, lengths, torch.Generator().manual_seed(42))\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for t_ds in train_datasets:\n",
    "        trainloaders.append(DataLoader(t_ds, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "\n",
    "    for v_ds in val_datasets:\n",
    "        valloaders.append(DataLoader(v_ds, batch_size=batch_size))\n",
    "        \n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import F1Score\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=14)\n",
    "\n",
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "\n",
    "    net.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            images, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            # images = images.expand(-1, 3, -1, -1)\n",
    "            outputs = net(images)\n",
    "            labels_ = labels.to(torch.float32)\n",
    "\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "\n",
    "    return epoch_loss, 0.0\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch\n",
    "            # images = images.expand(-1, 3, -1, -1)\n",
    "            # labels = labels.long()\n",
    "            labels = labels.to(torch.float32)\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "\n",
    "\n",
    "            total += labels.size(0)\n",
    "            # outputs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            true_classes = torch.argmax(labels, dim=1)\n",
    "\n",
    "            correct += torch.sum(predicted_classes == true_classes).item()\n",
    "\n",
    "\n",
    "\n",
    "    loss /= len(testloader.dataset)\n",
    "    # accuracy = metrics[1]\n",
    "    accuracy = correct / total\n",
    "    # accuracy = np.mean(accuracies)\n",
    "    return loss, accuracy * 100          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloaders, valloaders, testloader = load_datasets(3, batch_size=32)\n",
    "# net = Net(in_channels=1, num_classes=14)\n",
    "# train(net, trainloaders[0], 1)\n",
    "# test(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Net2(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super(Net2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(6 * 13 * 13, 500)\n",
    "#         self.fc2 = nn.Linear(500, 250)\n",
    "#         self.fc3 = nn.Linear(250, 120)\n",
    "#         self.fc4 = nn.Linear(120, 84)\n",
    "#         # self.fc5 = nn.Linear(120, 84)\n",
    "#         self.fc5 = nn.Linear(84, 2)\n",
    "\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         # x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "#         # print(x.shape)\n",
    "#         x = x.view(-1, 6 * 13 * 13)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)\n",
    "#         # x = F.softmax(x)\n",
    "#         return x\n",
    "# # Load model and data\n",
    "# # net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes)->None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "        {\n",
    "            k: torch.Tensor(v) if v.shape != torch.Size([]) else torch.Tensor([0])\n",
    "            for k, v in params_dict\n",
    "        }\n",
    "    )\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        # print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {\"loss\": float(loss), \"accuracy\": float(accuracy)}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        # print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"loss\": float(loss), \"accuracy\": float(accuracy)}\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    # net = Net().to(DEVICE)\n",
    "    net = Net(in_channels=1, num_classes=2).to(DEVICE)\n",
    "\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    # print(metrics)\n",
    "    min_accuracy = min([m['accuracy'] for _, m in metrics])\n",
    "    max_accuracy = max([m['accuracy'] for _, m in metrics])\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    # return {'accuracy': min(accuracies)}\n",
    "    return {\"average accuracy\": sum(accuracies) / sum(examples), \"min_accuracy\": min_accuracy, \"max_accuracy\": max_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    net = Net(in_channels=1, num_classes=2).to(DEVICE)\n",
    "    \n",
    "    # net = Net().to(DEVICE)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, testloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning Attacks Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_labels(poisoned_dataset, poison_rate=0.25):\n",
    "    # Calculate the number of samples to poison\n",
    "    num_samples = len(poisoned_dataset)\n",
    "    num_to_poison = int(num_samples * poison_rate)\n",
    "\n",
    "    # Randomly select samples to poison\n",
    "    indices_to_poison = np.random.choice(num_samples, num_to_poison, replace=False)\n",
    "    \n",
    "    for idx in indices_to_poison:\n",
    "        poisoned_dataset[idx] = 1 - poisoned_dataset[idx]\n",
    "\n",
    "def single_pixel_perturbations(poisoned_dataset, x, y, new_value, poison_rate=0.25):\n",
    "    \n",
    "    num_samples = len(poisoned_dataset)\n",
    "    num_to_poison = int(num_samples * poison_rate)\n",
    "\n",
    "    # Randomly select samples to poison\n",
    "    indices_to_poison = np.random.choice(num_samples, num_to_poison, replace=False)\n",
    "        \n",
    "    for idx in indices_to_poison:\n",
    "        poisoned_dataset[idx][0, y, x] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Sama\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Sama\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Sama\\.medmnist\\chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-01-25 15:29:56,737 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2024-01-25 15:30:02,148\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-01-25 15:30:06,226 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 1780618444.0, 'memory': 3561236891.0, 'node:127.0.0.1': 1.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}\n",
      "INFO flwr 2024-01-25 15:30:06,226 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-01-25 15:30:06,227 | app.py:227 | No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2024-01-25 15:30:06,228 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-01-25 15:30:06,243 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors\n",
      "INFO flwr 2024-01-25 15:30:06,244 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-01-25 15:30:06,245 | server.py:276 | Requesting initial parameters from one random client\n",
      "INFO flwr 2024-01-25 15:30:14,771 | server.py:280 | Received initial parameters from one random client\n",
      "INFO flwr 2024-01-25 15:30:14,771 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2024-01-25 15:30:25,735 | server.py:94 | initial parameters (loss, other metrics): 0.02168928379360589, {'accuracy': 53.171666740961975}\n",
      "INFO flwr 2024-01-25 15:30:25,736 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-01-25 15:30:25,737 | server.py:222 | fit_round 1: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.02168928379360589 / accuracy 53.171666740961975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-01-25 15:31:43,824 | server.py:236 | fit_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2024-01-25 15:31:43,848 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "INFO flwr 2024-01-25 15:31:56,915 | server.py:125 | fit progress: (1, 0.021521533326809752, {'accuracy': 53.28756742299291}, 91.17829430000006)\n",
      "DEBUG flwr 2024-01-25 15:31:56,916 | server.py:173 | evaluate_round 1: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.021521533326809752 / accuracy 53.28756742299291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-01-25 15:32:00,895 | server.py:187 | evaluate_round 1 received 3 results and 0 failures\n",
      "DEBUG flwr 2024-01-25 15:32:00,896 | server.py:222 | fit_round 2: strategy sampled 3 clients (out of 3)\n",
      "DEBUG flwr 2024-01-25 15:33:07,469 | server.py:236 | fit_round 2 received 3 results and 0 failures\n",
      "INFO flwr 2024-01-25 15:33:18,481 | server.py:125 | fit progress: (2, 0.019953382024262098, {'accuracy': 64.74390406989703}, 172.74472600000013)\n",
      "DEBUG flwr 2024-01-25 15:33:18,482 | server.py:173 | evaluate_round 2: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.019953382024262098 / accuracy 64.74390406989703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-01-25 15:33:22,173 | server.py:187 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2024-01-25 15:33:22,174 | server.py:222 | fit_round 3: strategy sampled 3 clients (out of 3)\n",
      "DEBUG flwr 2024-01-25 15:34:39,508 | server.py:236 | fit_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2024-01-25 15:34:52,612 | server.py:125 | fit progress: (3, 0.019616970776055558, {'accuracy': 65.68002496322383}, 266.8755622000001)\n",
      "DEBUG flwr 2024-01-25 15:34:52,613 | server.py:173 | evaluate_round 3: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.019616970776055558 / accuracy 65.68002496322383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-01-25 15:34:57,071 | server.py:187 | evaluate_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2024-01-25 15:34:57,072 | server.py:153 | FL finished in 271.33536690000005\n",
      "INFO flwr 2024-01-25 15:34:57,074 | app.py:226 | app_fit: losses_distributed [(1, 0.02149037970845535), (2, 0.019773867277535984), (3, 0.019501843008092566)]\n",
      "INFO flwr 2024-01-25 15:34:57,074 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-01-25 15:34:57,075 | app.py:228 | app_fit: metrics_distributed {'average accuracy': [(1, 54.496929744697425), (2, 65.7634666249726), (3, 66.25371278277757)], 'min_accuracy': [(1, 53.96952686447474), (2, 65.47205135062852), (3, 65.63252206472319)], 'max_accuracy': [(1, 54.88098422037978), (2, 66.22091468307035), (3, 67.07675849157528)]}\n",
      "INFO flwr 2024-01-25 15:34:57,075 | app.py:229 | app_fit: losses_centralized [(0, 0.02168928379360589), (1, 0.021521533326809752), (2, 0.019953382024262098), (3, 0.019616970776055558)]\n",
      "INFO flwr 2024-01-25 15:34:57,076 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 53.171666740961975), (1, 53.28756742299291), (2, 64.74390406989703), (3, 65.68002496322383)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.02149037970845535\n",
       "\tround 2: 0.019773867277535984\n",
       "\tround 3: 0.019501843008092566\n",
       "History (loss, centralized):\n",
       "\tround 0: 0.02168928379360589\n",
       "\tround 1: 0.021521533326809752\n",
       "\tround 2: 0.019953382024262098\n",
       "\tround 3: 0.019616970776055558\n",
       "History (metrics, distributed, evaluate):\n",
       "{'average accuracy': [(1, 54.496929744697425), (2, 65.7634666249726), (3, 66.25371278277757)], 'min_accuracy': [(1, 53.96952686447474), (2, 65.47205135062852), (3, 65.63252206472319)], 'max_accuracy': [(1, 54.88098422037978), (2, 66.22091468307035), (3, 67.07675849157528)]}History (metrics, centralized):\n",
       "{'accuracy': [(0, 53.171666740961975), (1, 53.28756742299291), (2, 64.74390406989703), (3, 65.68002496322383)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "NUM_CLIENTS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_ROUNDS = 3\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "CLIENT = 1  #0, 1, 2\n",
    "Poison_type = poison_labels # poison_labels / single_pixel_perturbations\n",
    "# Poison_type = single_pixel_perturbations\n",
    "Poison_rate = 1\n",
    "\n",
    "def modify_traindata(trainloaders, poison_type, poison_rate=1):\n",
    "    if(poison_type == single_pixel_perturbations):\n",
    "\n",
    "        for batch in (trainloaders):\n",
    "            single_pixel_perturbations(batch[0], 10, 10, torch.max(batch[0]), poison_rate=poison_rate)\n",
    "     \n",
    "    else:\n",
    "        for batch in (trainloaders):\n",
    "            poison_labels(batch[1], poison_rate)\n",
    "\n",
    "modify_traindata(trainloaders=trainloaders[0], poison_type=Poison_type, poison_rate=Poison_rate)\n",
    "modify_traindata(trainloaders=trainloaders[1], poison_type=Poison_type, poison_rate=Poison_rate)\n",
    "modify_traindata(trainloaders=trainloaders[2], poison_type=Poison_type, poison_rate=Poison_rate)\n",
    "\n",
    "server_config = fl.server.ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "# Pass parameters to the Strategy for server-side parameter initialization\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    # initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "    evaluate_metrics_aggregation_fn=weighted_average, \n",
    "    # fit_metrics_aggregation_fn=weighted_average,\n",
    "    evaluate_fn=evaluate\n",
    "    \n",
    ")\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=server_config,  \n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS, batch_size=BATCH_SIZE)\n",
    "# old_train = trainloaders[1]\n",
    "# modify_traindata(trainloaders=trainloaders[CLIENT], poison_type=single_pixel_perturbations, poison_rate=Poison_rate)\n",
    "# new_train = trainloaders[1]\n",
    "# for batch in old_train: \n",
    "#     print(batch[0][10])\n",
    "#     break\n",
    "# for batch in new_train: \n",
    "#     print(batch[0][10])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 batch \n",
    "# lr = 1e-3 \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.021634859852918794\n",
    "# \tround 2: 0.019494445754617267\n",
    "# \tround 3: 0.019551336893341268\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.021838079879740166\n",
    "# \tround 1: 0.021617193900751844\n",
    "# \tround 2: 0.01964001085544267\n",
    "# \tround 3: 0.01949963099403961\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 50.41448023590991), (2, 66.07548348123696), (3, 66.43182769728148)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 46.828333259038025), (1, 51.04979271608791), (2, 65.35461150982927), (3, 66.7186733829626)]}\n",
    "\n",
    "\n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.021248942525614416\n",
    "# \tround 2: 0.019625080685606092\n",
    "# \tround 3: 0.019320327407663318\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.02166293802515575\n",
    "# \tround 1: 0.02130382649956877\n",
    "# \tround 2: 0.01972372806997405\n",
    "# \tround 3: 0.019418413398981423\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 62.56357268082011), (2, 66.00401064781263), (3, 66.9222168388823)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 61.4897695359515), (2, 65.49280078455845), (3, 66.57602638969375)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64 batch \n",
    "# lr = 1e-3 \n",
    "\n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.010716385236471615\n",
    "# \tround 2: 0.00979215710741931\n",
    "# \tround 3: 0.009725512929264912\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.010840601371187312\n",
    "# \tround 1: 0.010640637767685297\n",
    "# \tround 2: 0.009730266535249719\n",
    "# \tround 3: 0.009642307873539136\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 65.28216410359872), (2, 66.73495334450656), (3, 66.7172567167055)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 64.39620202380422), (2, 66.59831498239201), (3, 66.89698212454866)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 batch \n",
    "# lr = 1e-3 \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.08661153506258784\n",
    "# \tround 2: 0.07887566295962155\n",
    "# \tround 3: 0.07774493999581282\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.0864690203638629\n",
    "# \tround 1: 0.08652663819157412\n",
    "# \tround 2: 0.0792080924776444\n",
    "# \tround 3: 0.07774718131632395\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 50.27183483503256), (2, 65.79912201753378), (3, 66.44974356690285)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 50.30981143850577), (2, 65.2877457317345), (3, 66.4244639593456)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32 \n",
    "# lr = 0.1 \n",
    "\n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.02237011717661033\n",
    "# \tround 2: 0.021585349851597994\n",
    "# \tround 3: 0.021599878324354702\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.021675996097438352\n",
    "# \tround 1: 0.02258573357783557\n",
    "# \tround 2: 0.02163072222347155\n",
    "# \tround 3: 0.021635136129023617\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 54.18494148519223), (2, 54.18494148519223), (3, 54.18494148519223)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 53.171666740961975), (2, 53.171666740961975), (3, 53.171666740961975)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poison labels \n",
    "# rate = 1 \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.021746917593043886\n",
    "# \tround 2: 0.020342148526587642\n",
    "# \tround 3: 0.019321635498937464\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.02170411833565569\n",
    "# \tround 1: 0.021733941010719307\n",
    "# \tround 2: 0.020360666946800884\n",
    "# \tround 3: 0.019343852770980552\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 45.81505851480777), (2, 65.97729412555603), (3, 66.5209876262538)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 46.828333259038025), (1, 46.828333259038025), (2, 65.52400481433602), (3, 66.5626532340748)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSP \n",
    "# rate = 1 \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.021658491245307273\n",
    "# \tround 2: 0.01963736343005101\n",
    "# \tround 3: 0.019270213384083426\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.02176191804614467\n",
    "# \tround 1: 0.021650527516035453\n",
    "# \tround 2: 0.01976930641970696\n",
    "# \tround 3: 0.019276142710216947\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 50.53925266230466), (2, 66.13783633154864), (3, 67.19848298005492)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 46.828333259038025), (1, 51.01858868631035), (2, 65.43485044354298), (3, 66.83011634645388)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 rounds trials \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.02150226518430445\n",
    "# \tround 2: 0.01944696508263232\n",
    "# \tround 3: 0.01931844058592802\n",
    "# \tround 4: 0.019155672249873926\n",
    "# \tround 5: 0.019090482891622185\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.02178466496197426\n",
    "# \tround 1: 0.02150007961363149\n",
    "# \tround 2: 0.019510929829904367\n",
    "# \tround 3: 0.01935488031444338\n",
    "# \tround 4: 0.019111589488059365\n",
    "# \tround 5: 0.019015373670002132\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 58.22254889253449), (2, 67.06476691722908), (3, 66.97566418177512), (4, 67.40351936025618), (5, 67.86701563291885)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 46.828333259038025), (1, 57.80323630365979), (2, 66.46012570766283), (3, 66.65626532340748), (4, 67.56118218695671), (5, 67.91779967012883)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poison labels 5 rounds \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.02132637792728052\n",
    "# \tround 2: 0.019729355985695637\n",
    "# \tround 3: 0.019349942204867\n",
    "# \tround 4: 0.019245311957510104\n",
    "# \tround 5: 0.019136189577066257\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.021630475113821566\n",
    "# \tround 1: 0.021393116159859532\n",
    "# \tround 2: 0.019856321979333187\n",
    "# \tround 3: 0.01943236189153613\n",
    "# \tround 4: 0.019296907409590513\n",
    "# \tround 5: 0.01909124156685098\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 55.06741602567626), (2, 65.63865606956561), (3, 66.62800623132914), (4, 67.08265895621781), (5, 66.99351332562507)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 53.8849017073062), (2, 64.89100878170552), (3, 66.2327820621406), (4, 66.8746935318504), (5, 67.72166005438417)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSP\n",
    "# rate = 1 \n",
    "# History (loss, distributed):\n",
    "# \tround 1: 0.021477125957758282\n",
    "# \tround 2: 0.01956604122161247\n",
    "# \tround 3: 0.01929507566124522\n",
    "# \tround 4: 0.01911810000906677\n",
    "# \tround 5: 0.01906080256308064\n",
    "# History (loss, centralized):\n",
    "# \tround 0: 0.02168635337493999\n",
    "# \tround 1: 0.021490031688832855\n",
    "# \tround 2: 0.0196927915761215\n",
    "# \tround 3: 0.0193582001228432\n",
    "# \tround 4: 0.019169284116566616\n",
    "# \tround 5: 0.019066446862282194\n",
    "# History (metrics, distributed, evaluate):\n",
    "# {'accuracy': [(1, 56.422216088217375), (2, 66.2091328182914), (3, 66.81517440317431), (4, 67.79567148491078), (5, 67.68870054110073)]}History (metrics, centralized):\n",
    "# {'accuracy': [(0, 53.171666740961975), (1, 55.984487139482006), (2, 65.57749743681184), (3, 66.43337939642491), (4, 67.29817679311728), (5, 67.42299291222751)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIM_EX1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
