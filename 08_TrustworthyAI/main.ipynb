{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc64114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853be59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d4f824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A3000 Laptop GPU\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#CUDA_VISIBLE_DEVICES=0\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.current_device())\n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288dd055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A3000 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a41919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training and privacy\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 5.0\n",
    "DELTA = 1e-5\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4a7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make the model differentially private\n",
    "def make_private(model, optimizer, train_loader, epsilon=EPSILON, epochs=EPOCHS):\n",
    "    model_dp, optimizer_dp, train_loader_dp = privacy_engine.make_private_with_epsilon(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        epochs=epochs,\n",
    "        target_epsilon=epsilon,\n",
    "        target_delta=DELTA,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "    )\n",
    "    return model_dp, optimizer_dp, train_loader_dp\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# Function for training the model\n",
    "def train(model, optimizer, train_loader, device, dp, task, epochs=EPOCHS): \n",
    "    start_time = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    \n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        losses = []\n",
    "        accs = []\n",
    "\n",
    "        # BatchMemoryManager manages the memory usage (e.g. you can use bigger logical batches)\n",
    "        with BatchMemoryManager(data_loader=train_loader, max_physical_batch_size=128, optimizer=optimizer) as loader:\n",
    "            if not dp:\n",
    "                loader = train_loader\n",
    "\n",
    "            for i, (images, target) in enumerate(loader):   \n",
    "                optimizer.zero_grad()\n",
    "                images = images.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                if task == 'multi_class': \n",
    "                    target = target.squeeze().float()\n",
    "                \n",
    "                output = model(images)\n",
    "                target = target.long()\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "                labels = target.detach().cpu().numpy()\n",
    "\n",
    "                acc = accuracy(preds, labels)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                accs.append(acc)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if dp:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "            else:\n",
    "                epsilon = float('inf')\n",
    "            print(\n",
    "                f\"Training Epoch {epoch+1:02d} \\t\"\n",
    "                f\"Loss: {np.mean(losses):.6f} | \"\n",
    "                f\"Acc: {np.mean(accs) * 100:.6f} | \"\n",
    "                f\"ε = {epsilon:.2f}\"\n",
    "            )\n",
    "                \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f'\\nTraining Time \\t\\t{training_time:.2f} seconds')\n",
    "                \n",
    "# Function for testing the model\n",
    "def test(model, test_loader, device, dp, task):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            if task == 'multi_class': \n",
    "                target = target.squeeze().float()\n",
    "                \n",
    "            output = model(images)\n",
    "            target = target.long()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "            \n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "\n",
    "    print(\n",
    "        f\"Test Set \\t\\t\"\n",
    "        f\"Loss: {np.mean(losses):.6f} | \"\n",
    "        f\"Acc: {np.mean(accs) * 100:.6f} \"\n",
    "    )\n",
    "\n",
    "# Function for the overall pipeline\n",
    "def pipeline(model, train_loader, test_loader, device, task=None):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "\n",
    "    # Specific layers of the model can't be made differentially private, so the model has to be fixed\n",
    "    model_fixed = ModuleValidator.fix(model)\n",
    "    optimizer_fixed = optim.RMSprop(model_fixed.parameters(), lr=LR)\n",
    "\n",
    "    model_dp, optimizer_dp, train_loader_dp = make_private(model_fixed, optimizer_fixed, train_loader)\n",
    "    \n",
    "    print(f'\\n===== With DP =====')\n",
    "    train(model_dp, optimizer_dp, train_loader_dp, device, True, task)\n",
    "    test(model_dp, test_loader, device, True, task)\n",
    "    print(f'\\n===== Without DP =====')\n",
    "    train(model, optimizer, train_loader, device, False, task)\n",
    "    test(model, test_loader, device, False, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/Users/valentinbiller/Downloads/data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/Users/valentinbiller/Downloads/data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize the privacy engine and the model\n",
    "privacy_engine = PrivacyEngine()\n",
    "model = torchvision.models.resnet18(num_classes=10)\n",
    "\n",
    "pipeline(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172bf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "root = '/Users/valentinbiller/Downloads/'\n",
    "datasets = ['dermamnist', 'pneumoniamnist', 'bloodmnist', 'organcmnist']\n",
    "\n",
    "transformMedMNIST = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5]),\n",
    "])\n",
    "\n",
    "for data_flag in datasets:\n",
    "    \n",
    "    info = INFO[data_flag]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    \n",
    "    train_dataset = DataClass(split='train', transform=transformMedMNIST, download=True, root=root)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    test_dataset = DataClass(split='test', transform=transformMedMNIST, download=True, root=root)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    num_classes = len(info['label'])\n",
    "    \n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "    if data_flag == 'pneumoniamnist' or data_flag == 'organcmnist':     \n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    " \n",
    "    print(f'\\n\\n\\n ========== {data_flag} ========== \\n')\n",
    "    pipeline(model, train_loader, test_loader, device, task='multi_class')\n",
    "    print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330b35d",
   "metadata": {},
   "source": [
    "## Task 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9f0a1",
   "metadata": {},
   "source": [
    "High Privacy (Level 1): Choose a lower target_epsilon value. target_epsilon = 0.1.\n",
    "Medium Privacy (Level 4): Choose a moderate target_epsilon value. target_epsilon = 1.\n",
    "Low Privacy (Level 8): Choose a higher target_epsilon value. target_epsilon = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595db2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO\n",
    "info = INFO[\"pneumoniamnist\"]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "num_classes = len(info['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8320a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\m1choelz\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\m1choelz\\.medmnist\\pneumoniamnist.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = [\"pneumoniamnist\"] #['dermamnist', 'pneumoniamnist', 'bloodmnist', 'organcmnist']\n",
    "\n",
    "transformMedMNIST = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5]),\n",
    "])\n",
    "\n",
    "# Initialize the privacy engine and the model\n",
    "privacy_engine = PrivacyEngine()\n",
    "model = torchvision.models.resnet18(num_classes=10)\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "lr = 1e-3\n",
    "  \n",
    "train_dataset = DataClass(split='train', transform=transformMedMNIST, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DataClass(split='test', transform=transformMedMNIST, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "# Define the model with adjustable parameters\n",
    "def build_model(num_layers, dropout_rate):\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    \n",
    "    # Modify the last fully connected layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(num_ftrs, 2)  # assuming binary classification\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Training and evaluation loop\n",
    "def train_and_evaluate(model, train_loader, test_loader, learning_rate, num_epochs=10, use_dp=False, dp_epsilon=1.0, dp_delta=1e-5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Attach Privacy Engine to the optimizer if using differential privacy\n",
    "    if use_dp:\n",
    "        privacy_engine = PrivacyEngine(\n",
    "            model,\n",
    "            batch_size=32,\n",
    "            sample_size=len(train_loader.dataset),\n",
    "            alphas=[10, 100],\n",
    "            noise_multiplier=1.0,\n",
    "            max_grad_norm=1.0,\n",
    "        )\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()  # regular SGD step\n",
    "        \n",
    "        # Evaluate the privacy budget spent so far\n",
    "        if use_dp:\n",
    "            epsilon, best_alpha = privacy_engine.get_privacy_spent(dp_delta)\n",
    "            print(f\"Train Epoch: {epoch + 1} \\t\"\n",
    "                  f\"Loss: {loss.item():.6f} \\t\"\n",
    "                  f\"(ε = {epsilon:.2f}, δ = {dp_delta}) for α = {best_alpha}\")\n",
    "        else:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy} %')\n",
    "    # Here you would implement a privacy assessment method\n",
    "    # This could involve differential privacy analysis or other techniques\n",
    "\n",
    "for num_layers in [18, 34, 50]:  # Different ResNet versions\n",
    "    for dropout_rate in [0.0, 0.5]:\n",
    "        for learning_rate in [0.001, 0.0001]:\n",
    "            print(f\"Training with num_layers: {num_layers}, dropout_rate: {dropout_rate}, learning_rate: {learning_rate}\")\n",
    "            model = build_model(num_layers, dropout_rate)\n",
    "            train_and_evaluate(model, train_loader, test_loader, learning_rate, use_dp=True, num_epochs=10)\n",
    "            # Add code here to evaluate privacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9994b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model2 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "model2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model3 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "model3.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f443ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the overall pipeline\n",
    "from torch.optim import RMSprop, Adam\n",
    "import copy \n",
    "def pipeline1c( train_loader, test_loader, device, task=None, epochs=EPOCHS, optim=(RMSprop), lr=LR):\n",
    "    level1 = 0.5\n",
    "    level2 = 4\n",
    "    level3 = 9\n",
    "\n",
    "   \n",
    "    model_fixed = ModuleValidator.fix(model)\n",
    "    model_fixed2 = ModuleValidator.fix(model2)\n",
    "    model_fixed3= ModuleValidator.fix(model3)\n",
    "    optimizer_fixed = optim(model_fixed.parameters(), lr=lr)\n",
    "    optimizer_fixed2 = optim(model_fixed2.parameters(), lr=lr)\n",
    "    optimizer_fixed3 = optim(model_fixed3.parameters(), lr=lr)\n",
    "    model_dp1, optimizer_dp1, train_loader_dp1 = make_private(model_fixed, optimizer_fixed, train_loader, level1, epochs)\n",
    "    model_dp2, optimizer_dp2, train_loader_dp2 = make_private(model_fixed2, optimizer_fixed2, train_loader, level2,epochs)\n",
    "    model_dp3, optimizer_dp3, train_loader_dp3 = make_private(model_fixed3, optimizer_fixed3, train_loader,level3,epochs)\n",
    "    print(f'\\n===== With DP level1 =====')\n",
    "    train(model_dp1, optimizer_dp1, train_loader_dp1, device, True, task,epochs)\n",
    "    test(model_dp1, test_loader, device, True, task)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n===== With DP level2 =====')\n",
    "    train(model_dp2, optimizer_dp2, train_loader_dp2, device, True, task,epochs)\n",
    "    test(model_dp2, test_loader, device, True, task)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n===== With DP level3 =====')\n",
    "    train(model_dp3, optimizer_dp3, train_loader_dp3, device, True, task,epochs)\n",
    "    test(model_dp3, test_loader, device, True, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "753e3fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== 64 ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n",
      "\n",
      "===== With DP level2 =====\n",
      "\n",
      "===== With DP level3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec2ae0d69184c53890e3b83d129a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.723913 | Acc: 71.840938 | ε = 4.26\n",
      "Training Epoch 02 \tLoss: 0.393171 | Acc: 88.548644 | ε = 5.09\n",
      "Training Epoch 03 \tLoss: 0.208835 | Acc: 94.389611 | ε = 5.75\n",
      "Training Epoch 04 \tLoss: 0.243192 | Acc: 93.968228 | ε = 6.32\n",
      "Training Epoch 05 \tLoss: 0.219192 | Acc: 94.886559 | ε = 6.84\n",
      "Training Epoch 06 \tLoss: 0.193264 | Acc: 95.461154 | ε = 7.32\n",
      "Training Epoch 07 \tLoss: 0.174646 | Acc: 95.575095 | ε = 7.77\n",
      "Training Epoch 08 \tLoss: 0.192411 | Acc: 95.498759 | ε = 8.20\n",
      "Training Epoch 09 \tLoss: 0.205574 | Acc: 95.434538 | ε = 8.60\n",
      "Training Epoch 10 \tLoss: 0.159306 | Acc: 96.217467 | ε = 9.00\n",
      "\n",
      "Training Time \t\t6138.37 seconds\n",
      "Test Set \t\tLoss: 0.968425 | Acc: 84.114583 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = [64, 128, 256]\n",
    "\n",
    "for batch_size in batch_size:\n",
    "    \n",
    "    privacy_engine = PrivacyEngine()\n",
    "    # to reset models after training: \n",
    "    model = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model2 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model3 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model3.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    " \n",
    "    print(f'\\n\\n\\n ========== {batch_size} ========== \\n')\n",
    "    pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "    print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce9dd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== resnet34 ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n",
      "\n",
      "===== With DP level2 =====\n",
      "\n",
      "===== With DP level3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b9718a62d1468e83ddfd7347782aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 2.070636 | Acc: 73.524336 | ε = 4.26\n",
      "Training Epoch 02 \tLoss: 1.265989 | Acc: 74.216246 | ε = 5.09\n",
      "Training Epoch 03 \tLoss: 0.349339 | Acc: 90.628885 | ε = 5.75\n",
      "Training Epoch 04 \tLoss: 0.269412 | Acc: 93.931942 | ε = 6.32\n",
      "Training Epoch 05 \tLoss: 0.238159 | Acc: 94.772765 | ε = 6.84\n",
      "Training Epoch 06 \tLoss: 0.221897 | Acc: 95.097371 | ε = 7.32\n",
      "Training Epoch 07 \tLoss: 0.208748 | Acc: 95.770681 | ε = 7.77\n",
      "Training Epoch 08 \tLoss: 0.216766 | Acc: 95.316235 | ε = 8.20\n",
      "Training Epoch 09 \tLoss: 0.183091 | Acc: 95.815597 | ε = 8.60\n",
      "Training Epoch 10 \tLoss: 0.206137 | Acc: 95.541217 | ε = 9.00\n",
      "\n",
      "Training Time \t\t6447.23 seconds\n",
      "Test Set \t\tLoss: 1.139144 | Acc: 82.477679 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "model = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model2 = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model3 = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model3.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "print(f'\\n\\n\\n ========== resnet34 ========== \\n')\n",
    "pipeline1c( train_loader, test_loader, device, task='multi_class', epochs = 10, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041b0083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== ADAM ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n",
      "\n",
      "===== With DP level2 =====\n",
      "\n",
      "===== With DP level3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426cc1ad694744cf906150166ae0207d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.227621 | Acc: 73.764304 | ε = 4.26\n",
      "Training Epoch 02 \tLoss: 0.459083 | Acc: 85.045902 | ε = 5.09\n",
      "Training Epoch 03 \tLoss: 0.287433 | Acc: 92.694988 | ε = 5.75\n",
      "Training Epoch 04 \tLoss: 0.239063 | Acc: 94.283932 | ε = 6.32\n",
      "Training Epoch 05 \tLoss: 0.209389 | Acc: 94.764335 | ε = 6.84\n",
      "Training Epoch 06 \tLoss: 0.179537 | Acc: 95.579621 | ε = 7.32\n",
      "Training Epoch 07 \tLoss: 0.169264 | Acc: 95.490683 | ε = 7.77\n",
      "Training Epoch 08 \tLoss: 0.166808 | Acc: 95.864017 | ε = 8.20\n",
      "Training Epoch 09 \tLoss: 0.168041 | Acc: 96.048149 | ε = 8.60\n",
      "Training Epoch 10 \tLoss: 0.164560 | Acc: 95.803203 | ε = 9.00\n",
      "\n",
      "Training Time \t\t2057.86 seconds\n",
      "Test Set \t\tLoss: 0.962280 | Acc: 83.258929 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "print(f'\\n\\n\\n ========== ADAM ========== \\n')\n",
    "pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = optim.Adam)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00dea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_alexnet(num_classes, input_channels=1):\n",
    "    # Load AlexNet\n",
    "    model = models.alexnet(pretrained=False)\n",
    "\n",
    "    # Modify the first convolution layer for 1-channel input\n",
    "    model.features[0] = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "\n",
    "    # Correct subsequent convolution layers to match the new channel sizes\n",
    "    model.features[3] = nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2)   # Second conv layer\n",
    "    model.features[6] = nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1)  # Third conv layer\n",
    "    model.features[8] = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)  # Fourth conv layer\n",
    "    model.features[10] = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # Fifth conv layer\n",
    "\n",
    "    # Modify the max pooling layers to be less aggressive\n",
    "    model.features[2] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)   # First max pooling layer\n",
    "    model.features[5] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)   # Second max pooling layer\n",
    "    model.features[12] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Third max pooling layer\n",
    "\n",
    "    # Modify the classifier for the number of classes\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ac7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== alexnet ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8727f4c03a6749fbbd1a2eb2dbe1d178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 9.00 GiB (GPU 0; 6.00 GiB total capacity; 13.44 GiB already allocated; 0 bytes free; 17.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model3 \u001b[38;5;241m=\u001b[39m modify_alexnet(num_classes)   \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ========== alexnet ========== \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mpipeline1c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRMSprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mpipeline1c\u001b[1;34m(train_loader, test_loader, device, task, epochs, optim, lr)\u001b[0m\n\u001b[0;32m     18\u001b[0m model_dp3, optimizer_dp3, train_loader_dp3 \u001b[38;5;241m=\u001b[39m make_private(model_fixed3, optimizer_fixed3, train_loader,level3,epochs)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== With DP level1 =====\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_dp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_dp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m test(model_dp1, test_loader, device, \u001b[38;5;28;01mTrue\u001b[39;00m, task)\n\u001b[0;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, device, dp, task, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     55\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dp:\n",
      "File \u001b[1;32mc:\\Users\\m1choelz\\AppData\\Local\\anaconda3\\envs\\medAI2\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m1choelz\\AppData\\Local\\anaconda3\\envs\\medAI2\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m1choelz\\AppData\\Local\\anaconda3\\envs\\medAI2\\lib\\site-packages\\opacus\\grad_sample\\grad_sample_module.py:358\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[1;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[0;32m    356\u001b[0m grad_samples \u001b[38;5;241m=\u001b[39m grad_sampler_fn(module, activations, backprops)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, gs \u001b[38;5;129;01min\u001b[39;00m grad_samples\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 358\u001b[0m     \u001b[43mcreate_or_accumulate_grad_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batch_len\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Detect end of current batch processing and switch accumulation\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# mode from sum to stacking. Used for RNNs and tied parameters\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# (See #417 for details)\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, p \u001b[38;5;129;01min\u001b[39;00m trainable_parameters(module):\n",
      "File \u001b[1;32mc:\\Users\\m1choelz\\AppData\\Local\\anaconda3\\envs\\medAI2\\lib\\site-packages\\opacus\\grad_sample\\grad_sample_module.py:55\u001b[0m, in \u001b[0;36mcreate_or_accumulate_grad_sample\u001b[1;34m(param, grad_sample, max_batch_len)\u001b[0m\n\u001b[0;32m     53\u001b[0m     param\u001b[38;5;241m.\u001b[39m_current_grad_sample[: grad_sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grad_sample\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     param\u001b[38;5;241m.\u001b[39m_current_grad_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmax_batch_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     param\u001b[38;5;241m.\u001b[39m_current_grad_sample[: grad_sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m grad_sample\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 9.00 GiB (GPU 0; 6.00 GiB total capacity; 13.44 GiB already allocated; 0 bytes free; 17.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "model = modify_alexnet(num_classes) \n",
    "\n",
    "model2 = modify_alexnet(num_classes)  \n",
    "\n",
    "model3 = modify_alexnet(num_classes)   \n",
    "\n",
    "print(f'\\n\\n\\n ========== alexnet ========== \\n')\n",
    "pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed9c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_vgg11(num_classes, input_channels=1):\n",
    "    # Load VGG11\n",
    "    model = models.vgg11(pretrained=False)\n",
    "\n",
    "    # Modify the first convolution layer for 1-channel input\n",
    "    model.features[0] = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "\n",
    "    # Modify the max pooling layers to be less aggressive\n",
    "    # VGG11 has pooling layers at indices 2, 5, 10, 15, and 20 in the 'features' module\n",
    "    model.features[2] = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)  # First pooling layer\n",
    "    model.features[5] = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)  # Second pooling layer\n",
    "    # Consider adjusting other pooling layers similarly\n",
    "\n",
    "    # Modify the classifier for the number of classes\n",
    "    num_features = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = modify_vgg11(num_classes, input_channels=1)  # Use input_channels=3 for RGB images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f'\\n\\n\\n ========== vgg11 ========== \\n')\n",
    "pipeline1c( train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c435c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\m1choelz\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\m1choelz\\.medmnist\\pneumoniamnist.npz\n",
      "\n",
      "\n",
      "\n",
      " ========== 0.0001 ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed5fe9683d444b9d10a2f993b4d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.335607 | Acc: 71.791420 | ε = 0.16\n",
      "Training Epoch 02 \tLoss: 1.159435 | Acc: 74.026861 | ε = 0.21\n",
      "Training Epoch 03 \tLoss: 0.852616 | Acc: 73.120730 | ε = 0.26\n",
      "Training Epoch 04 \tLoss: 0.622599 | Acc: 74.351145 | ε = 0.31\n",
      "Training Epoch 05 \tLoss: 0.475623 | Acc: 81.056603 | ε = 0.34\n",
      "Training Epoch 06 \tLoss: 0.368885 | Acc: 86.780325 | ε = 0.38\n",
      "Training Epoch 07 \tLoss: 0.271753 | Acc: 90.709477 | ε = 0.41\n",
      "Training Epoch 08 \tLoss: 0.249495 | Acc: 91.586975 | ε = 0.44\n",
      "Training Epoch 09 \tLoss: 0.229783 | Acc: 92.251053 | ε = 0.47\n",
      "Training Epoch 10 \tLoss: 0.239303 | Acc: 92.604443 | ε = 0.50\n",
      "\n",
      "Training Time \t\t1652.56 seconds\n",
      "Test Set \t\tLoss: 0.752382 | Acc: 80.171131 \n",
      "\n",
      "===== With DP level2 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b681f584175a4523b3e57578237a4555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.612388 | Acc: 72.685385 | ε = 1.80\n",
      "Training Epoch 02 \tLoss: 0.483016 | Acc: 84.585756 | ε = 2.17\n",
      "Training Epoch 03 \tLoss: 0.240172 | Acc: 93.192696 | ε = 2.48\n",
      "Training Epoch 04 \tLoss: 0.194279 | Acc: 94.871297 | ε = 2.75\n",
      "Training Epoch 05 \tLoss: 0.220001 | Acc: 94.717128 | ε = 3.00\n",
      "Training Epoch 06 \tLoss: 0.226218 | Acc: 94.901062 | ε = 3.23\n",
      "Training Epoch 07 \tLoss: 0.208229 | Acc: 95.027177 | ε = 3.45\n",
      "Training Epoch 08 \tLoss: 0.190505 | Acc: 95.638558 | ε = 3.65\n",
      "Training Epoch 09 \tLoss: 0.188565 | Acc: 95.539665 | ε = 3.85\n",
      "Training Epoch 10 \tLoss: 0.180785 | Acc: 95.567756 | ε = 4.03\n",
      "\n",
      "Training Time \t\t3042.72 seconds\n",
      "Test Set \t\tLoss: 0.954201 | Acc: 83.072917 \n",
      "\n",
      "===== With DP level3 =====\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "for lr in lr:\n",
    "    # to reset models after one training round\n",
    "    model = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model2 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model3 = torchvision.models.resnet18(num_classes=num_classes)    \n",
    "    model3.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "    info = INFO[\"pneumoniamnist\"]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    \n",
    "    train_dataset = DataClass(split='train', transform=transformMedMNIST, download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = DataClass(split='test', transform=transformMedMNIST, download=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    num_classes = len(info['label'])\n",
    "    \n",
    "    privacy_engine = PrivacyEngine()\n",
    "\n",
    "    print(f'\\n\\n\\n ========== {lr} ========== \\n')\n",
    "    pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "    print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1453f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== MAX_GRAD_NORM = 0.5 ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n",
      "\n",
      "===== With DP level2 =====\n",
      "\n",
      "===== With DP level3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6170620f894ef2ad85e637c6a2d837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.782303 | Acc: 72.350231 | ε = 4.26\n",
      "Training Epoch 02 \tLoss: 0.289944 | Acc: 91.913433 | ε = 5.09\n",
      "Training Epoch 03 \tLoss: 0.269135 | Acc: 94.075043 | ε = 5.75\n",
      "Training Epoch 04 \tLoss: 0.234353 | Acc: 94.603674 | ε = 6.32\n",
      "Training Epoch 05 \tLoss: 0.203162 | Acc: 95.371011 | ε = 6.84\n",
      "Training Epoch 06 \tLoss: 0.207649 | Acc: 95.562355 | ε = 7.32\n",
      "Training Epoch 07 \tLoss: 0.197224 | Acc: 95.606469 | ε = 7.77\n",
      "Training Epoch 08 \tLoss: 0.201211 | Acc: 95.567608 | ε = 8.20\n",
      "Training Epoch 09 \tLoss: 0.188878 | Acc: 95.786384 | ε = 8.60\n",
      "Training Epoch 10 \tLoss: 0.209565 | Acc: 95.748765 | ε = 9.00\n",
      "\n",
      "Training Time \t\t2923.50 seconds\n",
      "Test Set \t\tLoss: 1.113981 | Acc: 83.463542 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "print(f'\\n\\n\\n ========== MAX_GRAD_NORM = 0.5 ========== \\n')\n",
    "pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 1e-3\n",
    "privacy_engine = PrivacyEngine()\n",
    "model = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model2 = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model3 = torchvision.models.resnet34(num_classes=num_classes)    \n",
    "model3.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "print(f'\\n\\n\\n ========== {DELTA} ========== \\n')\n",
    "pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0787e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ========== squeezenet ========== \n",
      "\n",
      "\n",
      "===== With DP level1 =====\n",
      "\n",
      "===== With DP level2 =====\n",
      "\n",
      "===== With DP level3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed879d7e36aa41a2a8c5a9dafeec99dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 01 \tLoss: 1.486115 | Acc: 53.567863 | ε = 4.26\n",
      "Training Epoch 02 \tLoss: 1.461597 | Acc: 75.143074 | ε = 5.09\n",
      "Training Epoch 03 \tLoss: 1.179804 | Acc: 72.812274 | ε = 5.75\n",
      "Training Epoch 04 \tLoss: 1.026227 | Acc: 73.803154 | ε = 6.32\n",
      "Training Epoch 05 \tLoss: 0.849639 | Acc: 74.915985 | ε = 6.84\n",
      "Training Epoch 06 \tLoss: 0.755139 | Acc: 74.944098 | ε = 7.32\n",
      "Training Epoch 07 \tLoss: 0.747062 | Acc: 76.814854 | ε = 7.77\n",
      "Training Epoch 08 \tLoss: 0.655873 | Acc: 81.749970 | ε = 8.20\n",
      "Training Epoch 09 \tLoss: 0.656698 | Acc: 84.491463 | ε = 8.60\n",
      "Training Epoch 10 \tLoss: 0.760072 | Acc: 85.224640 | ε = 9.00\n",
      "\n",
      "Training Time \t\t57.54 seconds\n",
      "Test Set \t\tLoss: 0.809665 | Acc: 84.375000 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CustomSqueezeNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomSqueezeNet, self).__init__()\n",
    "        self.squeezenet = models.squeezenet1_0(pretrained=False)\n",
    "        self.squeezenet.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        # Add an additional 1x1 convolution to match the channel size\n",
    "        self.channel_matching_conv = nn.Conv2d(64, 96, kernel_size=1)\n",
    "\n",
    "        self.squeezenet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeezenet.features[0](x)\n",
    "        x = self.channel_matching_conv(x)  # Adjust the channels\n",
    "        x = self.squeezenet.features[1:](x)  # Continue through the rest of the SqueezeNet\n",
    "        x = self.squeezenet.classifier(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "model = CustomSqueezeNet(num_classes)\n",
    "model3 = CustomSqueezeNet(num_classes)\n",
    "model2 = CustomSqueezeNet(num_classes)\n",
    "print(f'\\n\\n\\n ========== squeezenet ========== \\n')\n",
    "pipeline1c(train_loader, test_loader, device, task='multi_class', epochs = epochs, optim = RMSprop)\n",
    "print(f'\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
