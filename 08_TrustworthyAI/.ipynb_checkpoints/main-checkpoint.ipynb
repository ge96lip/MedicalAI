{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc64114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from opacus import PrivacyEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define the size of the training set\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "valid_size = len(full_trainset) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "trainset, validset = torch.utils.data.random_split(full_trainset, [train_size, valid_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load the test set\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple off-the-shelf model (e.g., ResNet-18)\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(512, 10)  # Adapt output layer for CIFAR-10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Add Opacus Privacy Engine to the optimizer\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    sample_rate=0.01,  # Adjust as needed\n",
    "    target_epsilon=1.0,  # Adjust as needed\n",
    "    target_delta=1e-5,  # Adjust as needed\n",
    "    noise_multiplier=1.3,  # Adjust as needed\n",
    "    max_grad_norm=0.1,  # Adjust as needed\n",
    ")\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Train the model without DP\n",
    "optimizer_no_dp = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(optimizer): \n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf') \n",
    "    num_epochs = 10\n",
    "    # Train the model with DP\n",
    "    for epoch in range(10):  # Adjust the number of epochs\n",
    "        model.train()\n",
    "        tqdm_train_loader = tqdm(train_loader_chest_full, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n",
    "\n",
    "        for data in tqdm:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tqdm_train_loader.set_description(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, PSNR: {psnr_value.item():.4f}, SSIM: {ssim_value.item():.4f}')\n",
    "\n",
    "        tqdm_train_loader.close()  \n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "            val_running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(val_loader):\n",
    "                    # ... (validation code)\n",
    "                    inputs, targets = batch\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    # changing the target from 1D to 2D\n",
    "                    # if target == 1 new_target = [1, 0]\n",
    "                    # if target == 0 new_target = [0, 1]\n",
    "                    targets = targets.squeeze()\n",
    "                    target_2d = torch.zeros((len(targets), 2))\n",
    "                    target_2d[targets == 1, 0] = 1\n",
    "                    target_2d[targets == 0, 1] = 1\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = criterion(outputs, target_2d.float())\n",
    "\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Compute and append the average validation loss for the epoch\n",
    "            val_loss = val_running_loss / len(val_loader)\n",
    "            val_loss_values.append(val_loss)\n",
    "            print(f'Epoch [{epoch + 1}/{EPOCH_NUM}] - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "            # Check if the current validation loss is the best so far\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "        # Restore the model state with the lowest validation loss\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print results for the differentially private case\n",
    "    print(f'DP Accuracy: {100 * correct / total}%')\n",
    "    print(f'ε: {privacy_engine.get_privacy_spent()[0]}')\n",
    "    end_time = time.time()\n",
    "    dp_training_time = end_time - start_time\n",
    "    print(f'DP Training Time: {dp_training_time} seconds')\n",
    "\n",
    "    # show training and validation curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(loss_values) + 1), loss_values, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_loss_values) + 1), val_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):  # Adjust the number of epochs\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        optimizer_no_dp.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_no_dp.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct_no_dp = 0\n",
    "total_no_dp = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_no_dp += labels.size(0)\n",
    "        correct_no_dp += (predicted == labels).sum().item()\n",
    "\n",
    "# Print results for the non-private case\n",
    "print(f'Non-DP Accuracy: {100 * correct_no_dp / total_no_dp}%')\n",
    "print('ε: inf (non-private)')\n",
    "end_time = time.time()\n",
    "non_dp_training_time = end_time - start_time\n",
    "print(f'Non-DP Training Time: {non_dp_training_time} seconds')\n",
    "'''\n",
    "train_test(optimizerdp)\n",
    "train_test(optimizernondp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
